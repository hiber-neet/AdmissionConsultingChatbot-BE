from datetime import datetime
from typing import Any, Dict, List
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters  import RecursiveCharacterTextSplitter
from langchain_classic.memory import ConversationBufferMemory
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import os
import uuid
import asyncio
from sqlalchemy.orm import Session
from app.models import schemas
from app.models.entities import AcademicScore, ChatInteraction, ChatSession, FaqStatistics, Major, ParticipateChatSession, RiasecResult, TrainingQuestionAnswer
from app.models.database import SessionLocal
from sqlalchemy.exc import SQLAlchemyError
from app.services.memory_service import MemoryManager

memory_service = MemoryManager()

class TrainingService:
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(
            model="gpt-4.1-mini",
            api_key=self.openai_api_key,
            temperature=0.7
        )
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=self.openai_api_key
        )
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", 6333))
        )
        self.training_qa_collection = "training_qa"
        self.documents_collection = "knowledge_base_documents"
        self._init_collections()

    def _init_collections(self):
        try:
            self.qdrant_client.create_collection(
                collection_name=self.training_qa_collection,
                vectors_config=VectorParams(size=3072, distance=Distance.COSINE)
            )
        except:
            pass
            
        try:
            self.qdrant_client.create_collection(
                collection_name=self.documents_collection,
                vectors_config=VectorParams(size=3072, distance=Distance.COSINE)
            )
        except:
            pass

    def create_chat_session(self, user_id: int, session_type: str = "chatbot") -> int:
        """
        T·∫°o chat session m·ªõi
        
        Args:
            user_id: ID c·ªßa user
            session_type: "chatbot" ho·∫∑c "live"
        
        Returns:
            session_id: ID c·ªßa session v·ª´a t·∫°o
        """
        db = SessionLocal()
        try:
            session = ChatSession(
                session_type=session_type,
                start_time=datetime.now()
            )
            db.add(session)
            db.flush()
            
            # Add user v√†o participate table
            participate = ParticipateChatSession(
                user_id=user_id,
                session_id=session.chat_session_id
            )
            db.add(participate)
            db.commit()
            
            return session.chat_session_id
        except SQLAlchemyError as e:
            db.rollback()
            print(f"Error creating session: {e}")
            raise
        finally:
            db.close()

    def get_session_history(self, session_id: int, limit: int = 50) -> List[Dict]:
        """
        L·∫•y l·ªãch s·ª≠ chat c·ªßa session
        
        Returns:
            List of messages [{message_text, timestamp, is_from_bot}, ...]
        """
        db = SessionLocal()
        try:
            interactions = db.query(ChatInteraction).filter(
                ChatInteraction.session_id == session_id
            ).order_by(
                ChatInteraction.timestamp.asc()
            ).limit(limit).all()
            
            return [
                {
                    "message_text": i.message_text,
                    "timestamp": i.timestamp.isoformat() if i.timestamp else None,
                    "is_from_bot": i.is_from_bot,
                    "rating": i.rating
                }
                for i in interactions
            ]
        finally:
            db.close()
    
    def get_user_sessions(self, user_id: int) -> List[Dict]:
        """
        L·∫•y t·∫•t c·∫£ sessions c·ªßa user (ƒë·ªÉ hi·ªÉn th·ªã recent chats)
        
        Returns:
            List of sessions v·ªõi preview message cu·ªëi c√πng
        """
        db = SessionLocal()
        try:
            sessions = db.query(ChatSession).join(
                ParticipateChatSession
            ).filter(
                ParticipateChatSession.user_id == user_id
            ).order_by(
                ChatSession.start_time.desc()
            ).all()
            
            result = []
            for session in sessions:
                # L·∫•y message cu·ªëi c√πng l√†m preview
                last_msg = db.query(ChatInteraction).filter(
                    ChatInteraction.session_id == session.chat_session_id
                ).order_by(
                    ChatInteraction.timestamp.desc()
                ).first()
                
                result.append({
                    "session_id": session.chat_session_id,
                    "session_type": session.session_type,
                    "start_time": session.start_time.isoformat() if session.start_time else None,
                    "last_message_preview": last_msg.message_text[:50] + "..." if last_msg else "",
                    "last_message_time": last_msg.timestamp.isoformat() if last_msg and last_msg.timestamp else None
                })
            
            return result
        finally:
            db.close()

    # ---------------------------
    # Query enrichment: d√πng chat_history + last bot question ƒë·ªÉ build a full query
    # ---------------------------
    async def enrich_query(self, session_id: str, user_message: str) -> str:
        memory = memory_service.get_memory(session_id)
        mem_vars = memory.load_memory_variables({})
        chat_history = mem_vars.get("chat_history", "")

        prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω gi√∫p chuy·ªÉn c√°c c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng th√†nh c√°c truy v·∫•n t√¨m ki·∫øm ƒë·∫ßy ƒë·ªß cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.

        Cu·ªôc h·ªôi tho·∫°i g·∫ßn ƒë√¢y (theo th·ª© t·ª± t·ª´ c≈© ƒë·∫øn m·ªõi):
        {chat_history}

        Ph·∫£n h·ªìi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng: "{user_message}"

        Nhi·ªám v·ª•: D·ª±a tr√™n "cu·ªôc h·ªôi tho·∫°i g·∫ßn ƒë√¢y" v√† "ph·∫£n h·ªìi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng", b·∫°n h√£y ƒë·∫£m b·∫£o t·∫°o ra **m·ªôt c√¢u truy v·∫•n t√¨m ki·∫øm**, r√µ r√†ng, c·ª• th·ªÉ (b·∫±ng ti·∫øng Vi·ªát), th·ªÉ hi·ªán ƒë√∫ng √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ g·ª≠i cho chatbot rag t∆∞ v·∫•n ƒë·ªÉ n√≥ c√≥ th·ªÉ hi·ªÉu y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng. "Ch·ªâ t·∫°o truy v·∫•n n·∫øu ph·∫£n h·ªìi c·ªßa ng∆∞·ªùi d√πng l√† ph·∫ßn ti·∫øp n·ªëi ho·∫∑c l√†m r√µ n·ªôi dung trong h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥.", n·∫øu ph·∫£n h·ªìi c·ªßa ng∆∞·ªùi d√πng kh√¥ng tr·∫£ l·ªùi ho·∫∑c kh√¥ng li√™n quan cho cu·ªôc h·ªôi tho·∫°i g·∫ßn ƒë√¢y th√¨ h√£y tr·∫£ v·ªÅ y nguy√™n ph·∫£n h·ªìi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng.

        Ch·ªâ tr·∫£ v·ªÅ **m·ªôt d√≤ng truy v·∫•n duy nh·∫•t** (kh√¥ng th√™m n·ªôi dung kh√°c).  
        V√≠ d·ª•:
        - "Th√¥ng tin v·ªÅ ng√†nh C√¥ng ngh·ªá Th√¥ng tin t·∫°i tr∆∞·ªùng FPT"  
        - "H·ªçc ph√≠ ng√†nh CNTT h·ªá ch√≠nh quy nƒÉm 2025 t·∫°i tr∆∞·ªùng FPT"
        """
        # assume async predict exists
        enriched = await self.llm.ainvoke(prompt)
        # fallback: if empty use original
        enriched_txt = (enriched or "").strip().splitlines()[0] if enriched else user_message
        return enriched_txt   

    # ---------------------------
    # LLM relevance check: ensure enriched_query actually matches the training QA
    # ---------------------------
    async def llm_relevance_check(self, enriched_query: str, matched_question: str, answer: str) -> bool:
        prompt = f"""
        B·∫°n l√† chuy√™n gia ƒë√°nh gi√° gi·ªØa c√¢u h·ªèi t√¨m ki·∫øm, c√¢u h·ªèi trong c∆° s·ªü d·ªØ li·ªáu v√† c√¢u tr·∫£ l·ªùi cho 1 h·ªá th·ªëng chat RAG tuy·ªÉn sinh, h√£y suy lu·∫≠n. 

        C√¢u h·ªèi t√¨m ki·∫øm (ƒë√£ chu·∫©n h√≥a): "{enriched_query}"
        C√¢u h·ªèi DB: "{matched_question}"
        C√¢u tr·∫£ l·ªùi ch√≠nh th·ª©c: "{answer}"

        H√£y tr·∫£ l·ªùi duy nh·∫•t ch·ªâ m·ªôt t·ª´: "true" n·∫øu c√¢u h·ªèi DB ph√π h·ª£p v√† tr·∫£ l·ªùi ƒë√≥ h·ª£p l√Ω cho truy v·∫•n t√¨m ki·∫øm; "false" n·∫øu ch·ªâ tr√πng t·ª´ kh√≥a ho·∫∑c kh√¥ng ph√π h·ª£p.
        Ho·∫∑c c√≥ th·ªÉ tr·∫£ v·ªÅ "true" n·∫øu c√¢u h·ªèi t√¨m ki·∫øm ch·ªâ l√† l·ªùi ch√†o.
        """
        res = await self.llm.ainvoke(prompt)
        if not res:
            return False
        r = res.strip().lower()
        return ("ƒë√∫ng" in r) or ("true" in r) or (r.startswith("ƒë√∫ng")) or (r.startswith("true"))

    async def llm_document_recommendation_check(self, enriched_query: str, context: str) -> bool:
        prompt = f"""
        B·∫°n l√† h·ªá th·ªëng ki·ªÉm tra 2 t·∫ßng:
        - T·∫ßng 1 l√† h·ªá th·ªëng ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi ng∆∞·ªùi d√πng v√† n·ªôi dung trong Document Base (RAG) cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.
        - T·∫ßng 2 l√† h·ªá th·ªëng ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung t∆∞ v·∫•n ng√†nh h·ªçc hay t∆∞ v·∫•n cho c√° nh√¢n d·ª±a theo h·ªì s∆° c·ªßa h·ªçc sinh ho·∫∑c nh·ªØng c√¢u li√™n quan ƒë·∫øn RIASEC, h·ªçc b·∫°, GPA, s·ªü th√≠ch, nguy·ªán v·ªçng c√° nh√¢n; ho·∫∑c y√™u c·∫ßu so s√°nh ng√†nh theo profile; ho·∫∑c y√™u c·∫ßu g·ª£i √Ω ng√†nh ph√π h·ª£p cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.
        Y√™u c·∫ßu ki·ªÉm tra c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ ph√π h·ª£p v·ªõi t·∫ßng 1 ho·∫∑c t·∫ßng 2:
        - N·∫øu ph√π h·ª£p v·ªõi t·∫ßng 1 th√¨ tr·∫£ v·ªÅ duy nh·∫•t 1 t·ª´ "document"
        - N·∫øu ph√π h·ª£p v·ªõi t·∫ßng 2 th√¨ tr·∫£ v·ªÅ duy nh·∫•t 1 t·ª´ "recommendation"
        - N·∫øu t·∫ßng 1 ph√π h·ª£p th√¨ kh√¥ng c·∫ßn check ƒë·∫øn t·∫ßng 2
        - N·∫øu kh√¥ng ph√π h·ª£p v·ªõi t·∫ßng 1 v√† t·∫ßng 2 th√¨ tr·∫£ v·ªÅ duy nh·∫•t 1 t·ª´ "Nope"
        - Check t·∫ßng 1(document) ƒë·∫ßu ti√™n:
        - Ch·ªâ tr·∫£ v·ªÅ "document" n·∫øu N·ªòI DUNG c·ªßa document base TH·ª∞C S·ª∞ c√≥ th√¥ng tin tr·∫£ l·ªùi c√¢u h·ªèi ho·∫∑c c√¢u h·ªèi ng∆∞·ªùi d√πng ch·ªâ l√† nh·ªØng l·ªùi ch√†o.
        - Check qua t·∫ßng 2 n·∫øu:
            ‚Ä¢ ch·ªâ tr√πng t·ª´ kh√≥a nh∆∞ng kh√¥ng c√πng √Ω nghƒ©a
            ‚Ä¢ document kh√¥ng ch·ª©a d·ªØ li·ªáu c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi
            ‚Ä¢ truy v·∫•n l√† y√™u c·∫ßu t∆∞ v·∫•n c√° nh√¢n (Recommendation), kh√¥ng ph·∫£i t√¨m ki·∫øn th·ª©c
            ‚Ä¢ query chung chung nh∆∞: "t√¥i h·ª£p ng√†nh n√†o", "h√£y t∆∞ v·∫•n", "m√¥ t·∫£ v·ªÅ t√¥i", "n√™n h·ªçc g√¨"
            ‚Ä¢ context kh√¥ng cung c·∫•p th√¥ng tin tr·ª±c ti·∫øp li√™n quan
        - Check t·∫ßng 2(recommendation):
        - Ch·ªâ tr·∫£ v·ªÅ "recommendation" n·∫øu c√¢u h·ªèi ng∆∞·ªùi d√πng li√™n quan ƒë·∫øn c√°c n·ªôi dung t∆∞ v·∫•n ng√†nh h·ªçc hay t∆∞ v·∫•n cho c√° nh√¢n d·ª±a theo h·ªì s∆° c·ªßa h·ªçc sinh ho·∫∑c nh·ªØng c√¢u li√™n quan ƒë·∫øn RIASEC, h·ªçc b·∫°, GPA, s·ªü th√≠ch, nguy·ªán v·ªçng c√° nh√¢n; ho·∫∑c y√™u c·∫ßu so s√°nh ng√†nh theo profile; ho·∫∑c y√™u c·∫ßu g·ª£i √Ω ng√†nh ph√π h·ª£p
        - Ch·ªâ tr·∫£ v·ªÅ "Nope" khi c·∫£ t·∫ßng 1 v√† t·∫ßng 2 ƒë·ªÅu kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi ng∆∞·ªùi d√πng.
        
        C√¢u h·ªèi ng∆∞·ªùi d√πng: "{enriched_query}"

        N·ªôi dung Document Base (context):
        \"\"\"
        {context}
        \"\"\"

        
        """

        res = await self.llm.ainvoke(prompt)
        r = res.strip().lower()
        if r not in ["document", "recommendation", "nope"]:
            r = "nope"
        return r

    async def llm_suitable_for_recommedation_check(self, enriched_query: str, context: str) -> bool:
        prompt = f"""
        B·∫°n l√† h·ªá th·ªëng ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung t∆∞ v·∫•n ng√†nh h·ªçc hay t∆∞ v·∫•n cho c√° nh√¢n d·ª±a theo h·ªì s∆° c·ªßa h·ªçc sinh ho·∫∑c nh·ªØng c√¢u li√™n quan ƒë·∫øn RIASEC, h·ªçc b·∫°, GPA, s·ªü th√≠ch, nguy·ªán v·ªçng c√° nh√¢n; ho·∫∑c y√™u c·∫ßu so s√°nh ng√†nh theo profile; ho·∫∑c y√™u c·∫ßu g·ª£i √Ω ng√†nh ph√π h·ª£p cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.

        Y√™u c·∫ßu:
        - Ch·ªâ tr·∫£ v·ªÅ "true" n·∫øu c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥.
        - Tr·∫£ v·ªÅ "false" n·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥.

        C√¢u h·ªèi ng∆∞·ªùi d√πng: "{enriched_query}"

        
        H√£y TR·∫¢ L·ªúI DUY NH·∫§T:
        - "true" ‚Üí n·∫øu c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥ 
        - "false" ‚Üí n·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥
        """

        res = await self.llm.ainvoke(prompt)
        if not res:
            return False
        r = res.strip().lower()
        return ("ƒë√∫ng" in r) or ("true" in r) or (r.startswith("ƒë√∫ng")) or (r.startswith("true"))

    async def response_from_riasec_result(self, riasec_result: schemas.RiasecResultCreate):
        prompt = f"""
        B·∫°n l√† chuy√™n gia h∆∞·ªõng nghi·ªáp Holland (RIASEC).

        D∆∞·ªõi ƒë√¢y l√† ƒëi·ªÉm RIASEC c·ªßa ng∆∞·ªùi d√πng:
        - Realistic (R): {riasec_result.score_realistic}
        - Investigative (I): {riasec_result.score_investigative}
        - Artistic (A): {riasec_result.score_artistic}
        - Social (S): {riasec_result.score_social}
        - Enterprising (E): {riasec_result.score_enterprising}
        - Conventional (C): {riasec_result.score_conventional}

        Y√™u c·∫ßu:
        1. T·ª± x√°c ƒë·ªãnh m√£ RIASEC ch√≠nh c·ªßa ng∆∞·ªùi d√πng b·∫±ng c√°ch ch·ªçn 3 nh√≥m c√≥ ƒëi·ªÉm cao nh·∫•t (v√≠ d·ª•: ‚ÄúISA‚Äù, ‚ÄúREI‚Äù, ‚ÄúSEC‚Äù‚Ä¶).
        2. Gi·∫£i th√≠ch √Ω nghƒ©a m√£ RIASEC ƒë√≥ theo phong c√°ch h∆∞·ªõng nghi·ªáp.
        3. T√≥m t·∫Øt ƒë·∫∑c ƒëi·ªÉm t√≠nh c√°ch ch√≠nh (3‚Äì5 c√¢u).
        4. Vi·∫øt s√∫c t√≠ch, d·ªÖ hi·ªÉu, kh√¥ng d√πng markdown v√† ch·ªâ tr·∫£ v·ªÅ duy nh·∫•t m·ªôt ƒëo·∫°n vƒÉn.

        Tr·∫£ v·ªÅ:
        - M·ªôt ƒëo·∫°n vƒÉn ho√†n ch·ªânh, bao g·ªìm c·∫£ m√£ RIASEC m√† b·∫°n suy lu·∫≠n.
            """

        try:
            res = await self.llm.ainvoke(prompt)
            return res.strip()

        except Exception as e:
            print("LLM error:", e)
            return "Xin l·ªói, h·ªá th·ªëng t·∫°m th·ªùi ch∆∞a th·ªÉ ph√¢n t√≠ch k·∫øt qu·∫£ RIASEC. B·∫°n vui l√≤ng th·ª≠ l·∫°i sau."

    async def load_session_history_to_memory(self, session_id: int, db: Session):
        memory = memory_service.get_memory(session_id)

        # L·∫•y l·ªãch s·ª≠ chat theo th·ª© t·ª± th·ªùi gian
        interactions = (
            db.query(ChatInteraction)
            .filter(ChatInteraction.session_id == session_id)
            .order_by(ChatInteraction.timestamp.asc())
            .all()
        )

        last_user_msg = None
        for inter in interactions:
            if not inter.is_from_bot:
                # user message
                last_user_msg = inter.message_text
            else:
                # bot message -> k·∫øt h·ª£p v·ªõi user message tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥)
                memory.save_context(
                    {"input": last_user_msg or ""},
                    {"output": inter.message_text}
                )
                last_user_msg = None

        # N·∫øu cu·ªëi c√πng l√† tin nh·∫Øn user ch∆∞a ƒë∆∞·ª£c ph·∫£n h·ªìi
        if last_user_msg:
            memory.save_context({"input": last_user_msg}, {"output": ""})

    def update_faq_statistics(self, db: Session, question_text: str, answer_text: str, intent_id: int = 1):
        """
        TƒÉng usage_count cho m·ªôt Q&A ƒë√£ d√πng (Tier 1).
        - T·∫°o m·ªõi n·∫øu ch∆∞a c√≥.
        - C·∫≠p nh·∫≠t usage_count v√† last_used_at n·∫øu ƒë√£ t·ªìn t·∫°i.
        """
        try:
            faq_stat = db.query(FaqStatistics).filter(FaqStatistics.intent_id == intent_id).first()

            if faq_stat:
                # C·∫≠p nh·∫≠t n·∫øu ƒë√£ t·ªìn t·∫°i
                faq_stat.usage_count =  (faq_stat.usage_count or 0) + 1
                faq_stat.last_used_at = datetime.now()
            else:
                # T·∫°o m·ªõi n·∫øu ch∆∞a t·ªìn t·∫°i
                new_stat = FaqStatistics(
                    usage_count=1,
                    success_rate=None,
                    question_text=question_text,  # Placeholder
                    answer_text=answer_text,      # Placeholder
                    rating=None,
                    last_used_at=datetime.now(),
                    intent_id=intent_id
                )
                db.add(new_stat)

            db.commit()
            

        except Exception as e:
            db.rollback()
            print(f"Error updating FaqStatistics: {e}")
            

    async def stream_response_from_context(self, query: str, context: str, session_id: int = 1, user_id: int = 1, intent_id: int = 1):
        db = SessionLocal()
        
        try:
            # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
            user_msg = ChatInteraction(
                message_text=query,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=False,
                sender_id=user_id,
                session_id=session_id
            )
            db.add(user_msg)
            db.flush()  # flush ƒë·ªÉ l·∫•y ID n·∫øu c·∫ßn li√™n k·∫øt sau
        
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")
            """Stream ph·∫£n h·ªìi t·ª´ Gemini, t·ª´ng chunk m·ªôt."""
            prompt = f"""B·∫°n l√† m·ªôt chatbot t∆∞ v·∫•n tuy·ªÉn sinh chuy√™n nghi·ªáp c·ªßa tr∆∞·ªùng FPT
            ƒê√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i tr∆∞·ªõc: 
            {chat_history}
            === TH√îNG TIN THAM KH·∫¢O ===
            {context}
            === C√ÇU H·ªéI ===
            {query}
            === H∆Ø·ªöNG D·∫™N ===
            - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
            - Th√¢n thi·ªán, chuy√™n nghi·ªáp
            - D·ª±a v√†o th√¥ng tin tham kh·∫£o tr√™n ƒë∆∞·ª£c cung c·∫•p
            - B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng FPT, n·∫øu th√¥ng tin c√¢u h·ªèi y√™u c√¢u t√™n 1 tr∆∞·ªùng kh√°c th√¨ h√£y n√≥i r√µ ra l√† kh√¥ng t√¨m th·∫•y th√¥ng tin
            - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ v√† g·ª£i √Ω li√™n h·ªá tr·ª±c ti·∫øp nh√¢n vi√™n t∆∞ v·∫•n
            - Kh√¥ng b·ªãa th√™m th√¥ng tin ngo√†i context
            - N·∫øu c√¢u h·ªèi ch·ªâ l√† ch√†o h·ªèi, ho·∫∑c c√°c c√¢u x√£ giao, h√£y tr·∫£ l·ªùi b·∫±ng l·ªùi ch√†o th√¢n thi·ªán, gi·ªõi thi·ªáu v·ªÅ b·∫£n th√¢n chatbot, KH√îNG k√©o th√™m th√¥ng tin chi ti·∫øt trong context.
            - C√≥ th·ªÉ **di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi ho·∫∑c th√¥ng tin** m·ªôt c√°ch nh·∫π nh√†ng, t·ª± nhi√™n ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ hi·ªÉu h∆°n, **nh∆∞ng tuy·ªát ƒë·ªëi kh√¥ng thay ƒë·ªïi √Ω nghƒ©a hay th√™m d·ªØ ki·ªán m·ªõi.**
            - Khi c√≥ th·ªÉ, h√£y **gi·∫£i th√≠ch th√™m b·ªëi c·∫£nh ho·∫∑c g·ª£i √Ω b∆∞·ªõc ti·∫øp theo**, v√≠ d·ª•:  
                ‚ÄúB·∫°n mu·ªën m√¨nh g·ª≠i danh s√°ch ng√†nh ƒë√†o t·∫°o k√®m chuy√™n ng√†nh chi ti·∫øt kh√¥ng?‚Äù  
                ho·∫∑c  
                ‚ÄúN·∫øu b·∫°n quan t√¢m h·ªçc b·ªïng, m√¨nh c√≥ th·ªÉ n√≥i r√µ c√°c lo·∫°i h·ªçc b·ªïng hi·ªán c√≥ nh√©!‚Äù
            """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                text = chunk.content or ""
                full_response += text
                yield text
                await asyncio.sleep(0)  # Nh∆∞·ªùng event loop
            memory.save_context({"input": query}, {"output": full_response})  
            
            # === üî• L∆∞u bot response v√†o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)



            # üß© 5. Commit 1 l·∫ßn duy nh·∫•t
            db.commit()
            self.update_faq_statistics(db, question_text = query, answer_text = full_response, intent_id = intent_id)
            print(f"üíæ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close()

    async def stream_response_from_qa(self, query: str, context: str, session_id: int = 1, user_id: int = 1, intent_id: int = 1):
        db = SessionLocal()
        try:
            # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
            user_msg = ChatInteraction(
                message_text=query,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=False,
                sender_id=user_id,
                session_id=session_id
            )
            db.add(user_msg)
            db.flush()  # flush ƒë·ªÉ l·∫•y ID n·∫øu c·∫ßn li√™n k·∫øt sau
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")

            prompt = f"""
            B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng FPT.
            ƒê√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i tr∆∞·ªõc: 
            {chat_history}
            === C√ÇU TR·∫¢ L·ªúI CH√çNH TH·ª®C ===
            {context}

            === C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG ===
            {query}

            === H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI ===
            - H√£y ƒë·ªçc k·ªπ ph·∫ßn NG·ªÆ C·∫¢NH LI√äN QUAN, nh∆∞ng **ch·ªâ s·ª≠ d·ª•ng n√≥ n·∫øu th·∫≠t s·ª± c√≥ n·ªôi dung tr√πng kh·ªõp ho·∫∑c ph√π h·ª£p v·ªõi c√¢u h·ªèi ng∆∞·ªùi d√πng.**
            - N·∫øu ph·∫ßn C√ÇU TR·∫¢ L·ªúI CH√çNH TH·ª®C kh√¥ng li√™n quan r√µ r√†ng ƒë·∫øn c√¢u h·ªèi, **ƒë·ª´ng c·ªë tr·∫£ l·ªùi theo context** m√† h√£y n√≥i:
            ‚ÄúHi·ªán ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c cho c√¢u h·ªèi n√†y. B·∫°n c√≥ th·ªÉ n√≥i r√µ chi ti·∫øt h∆°n ƒë∆∞·ª£c kh√¥ng?‚Äù 
            - N·∫øu ph·∫ßn tr·∫£ l·ªùi ch√≠nh th·ª©c kh√¥ng ph√π h·ª£p v·ªõi c√¢u h·ªèi, h√£y n√≥i ‚ÄúHi·ªán ch∆∞a c√≥ th√¥ng tin cho c√¢u h·ªèi n√†y. Vui l√≤ng li√™n h·ªá chuy√™n vi√™n t∆∞ v·∫•n.‚Äù
            - B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng FPT, nh·ªõ ki·ªÉm tra kƒ© r√µ r√†ng c√¢u h·ªèi, n·∫øu th√¥ng tin c√¢u h·ªèi y√™u c√¢u t√™n 1 tr∆∞·ªùng kh√°c th√¨ h√£y n√≥i r√µ ra l√† kh√¥ng t√¨m th·∫•y th√¥ng tin
            - N·∫øu c√¢u h·ªèi ch·ªâ l√† ch√†o h·ªèi, h·ªèi th·ªùi ti·∫øt, ho·∫∑c c√°c c√¢u x√£ giao, h√£y tr·∫£ l·ªùi b·∫±ng l·ªùi ch√†o th√¢n thi·ªán, gi·ªõi thi·ªáu v·ªÅ b·∫£n th√¢n chatbot, KH√îNG k√©o th√™m th√¥ng tin chi ti·∫øt trong context.
            - N·∫øu c√¢u h·ªèi qu√° m∆° h·ªì, h√£y h·ªèi l·∫°i ƒë·ªÉ r√µ h∆°n v√† chi ti·∫øt h∆°n v·ªÅ c√¢u h·ªèi
            - C√≥ th·ªÉ **di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi ho·∫∑c th√¥ng tin** m·ªôt c√°ch nh·∫π nh√†ng, t·ª± nhi√™n ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ hi·ªÉu h∆°n, **nh∆∞ng tuy·ªát ƒë·ªëi kh√¥ng thay ƒë·ªïi √Ω nghƒ©a hay th√™m d·ªØ ki·ªán m·ªõi.**
            - Khi c√≥ th·ªÉ, h√£y **gi·∫£i th√≠ch th√™m b·ªëi c·∫£nh ho·∫∑c g·ª£i √Ω b∆∞·ªõc ti·∫øp theo**, v√≠ d·ª•:  
                ‚ÄúB·∫°n mu·ªën m√¨nh g·ª≠i danh s√°ch ng√†nh ƒë√†o t·∫°o k√®m chuy√™n ng√†nh chi ti·∫øt kh√¥ng?‚Äù  
                ho·∫∑c  
                ‚ÄúN·∫øu b·∫°n quan t√¢m h·ªçc b·ªïng, m√¨nh c√≥ th·ªÉ n√≥i r√µ c√°c lo·∫°i h·ªçc b·ªïng hi·ªán c√≥ nh√©!‚Äù
            """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                text = chunk.content or ""
                full_response += text
                yield text
                await asyncio.sleep(0)  # Nh∆∞·ªùng event loop

            memory.save_context({"input": query}, {"output": full_response})  
            print("Saved to memory. Current messages:", len(memory.chat_memory.messages))

            # === L∆∞u bot response v√†o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)

            # üß© 5. Commit 1 l·∫ßn duy nh·∫•t
            db.commit()
            self.update_faq_statistics(db, question_text = query, answer_text = full_response, intent_id = intent_id)
            print(f"üíæ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close() 
    
    async def stream_response_from_recommendation(
        self,
        user_id: int,
        session_id: int,
        query: str
    ):
        db = SessionLocal()
        try:
            # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
            user_msg = ChatInteraction(
                message_text=query,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=False,
                sender_id=user_id,
                session_id=session_id
            )
            db.add(user_msg)
            db.flush()  # flush ƒë·ªÉ l·∫•y ID n·∫øu c·∫ßn li√™n k·∫øt sau
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")

            user_profile = self._get_user_personality_and_academics(user_id, db)
            majors = self._get_all_majors_from_db(db, limit=200)

            personality = user_profile.get("personality_summary") or ""
            academic_summary = user_profile.get("academic_summary") or ""
            gpa = user_profile.get("gpa", "")

            # R√∫t g·ªçn danh s√°ch ng√†nh
            maj_texts = []
            for m in majors:
                maj_texts.append(f"- [{m['major_id']}]: {m['major_name']}")

            prompt = f"""
        B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc FPT. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t∆∞ v·∫•n ch·ªçn ng√†nh:
        **CH·ªà t∆∞ v·∫•n ch·ªçn ng√†nh khi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th·∫≠t s·ª± li√™n quan.**
        
        ƒê√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i tr∆∞·ªõc: 
            {chat_history}
        ===========================
        ### TH√îNG TIN H·ªí S∆† NG∆Ø·ªúI D√ôNG
        Personality summary(RIASEC Result):
        {personality}

        Academic summary(h·ªçc b·∫°):
        {academic_summary}

        

        ===========================
        ### DANH S√ÅCH C√ÅC NG√ÄNH
        {chr(10).join(maj_texts)}

        ===========================
        ### C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG
        "{query}"

        ===========================
        ### H∆Ø·ªöNG D·∫™N X·ª¨ L√ù

        1. **ƒê·∫ßu ti√™n, h√£y ki·ªÉm tra xem c√¢u h·ªèi c√≥ th·∫≠t s·ª± li√™n quan ƒë·∫øn vi·ªác t∆∞ v·∫•n ch·ªçn ng√†nh hay kh√¥ng, ho·∫∑c c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn th√¥ng tin h·ªì s∆° ng∆∞·ªùi d√πng hay kh√¥ng.**
        - N·∫øu KH√îNG li√™n quan ‚Üí b·∫°n h√£y t·ª± t·∫°o c√¢u ph·∫£n h·ªìi ph√π h·ª£p v·ªõi C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG
        2. N·∫øu c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn th√¥ng tin h·ªì s∆° ng∆∞·ªùi d√πng ·ªü tr√™n bao g·ªìm RIASEC Result v√† h·ªçc b·∫° m√† h·ªì s∆° ng∆∞·ªùi d√πng tr·ªëng th√¨ h√£y y√™u c·∫ßu ng∆∞·ªùi d√πng nh·∫≠p nh·ªØng th√¥ng tin n√†y nh∆∞ RIASEC Result ho·∫∑c h·ªçc b·∫°, 1 trong 2 l√† c√≥ th·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n d·ª±a v√†o th√¥ng tin h·ªì s∆° ng∆∞·ªùi d√πng. ƒê·ªÅ xu·∫•t theo t√≠nh c√°ch c√≥ th·ªÉ d·ª±a v√†o k·∫øt qu·∫£ RIASEC Result c·ªßa TH√îNG TIN H·ªí S∆† NG∆Ø·ªúI D√ôNG
        3. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan th√¨ h√£y t·ª´ ch·ªëi y√™u c·∫ßu v√† ƒë·ªÅ ngh·ªã nh·∫Øn tr·ª±c ti·∫øp b√™n tuy·ªÉn sinh
    
        """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                text = chunk.content or ""
                full_response += text
                yield text
                await asyncio.sleep(0)  # Nh∆∞·ªùng event loop

            memory.save_context({"input": query}, {"output": full_response})  
            print("Saved to memory. Current messages:", len(memory.chat_memory.messages))

            # === L∆∞u bot response v√†o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)

            # üß© 5. Commit 1 l·∫ßn duy nh·∫•t
            db.commit()
            print(f"üíæ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close()

    def add_document(self, document_id: int, content: str, intend_id: int, metadata: dict = None):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # Size optimal cho Vietnamese
            chunk_overlap=200     # Overlap to preserve context
        )
        chunks = text_splitter.split_text(content)
        
        chunk_ids = []
        for i, chunk in enumerate(chunks):
            # Embed chunk
            embedding = self.embeddings.embed_query(chunk)
            point_id = str(uuid.uuid4())
            
            # Upsert to Qdrant
            self.qdrant_client.upsert(
                collection_name="knowledge_base_documents",
                points=[
                    PointStruct(
                        id=point_id,
                        vector=embedding,
                        payload={
                            "document_id": document_id,
                            "chunk_index": i,
                            "chunk_text": chunk,
                            "intend_id": intend_id,
                            "metadata": metadata or {},
                            "type": "document"
                        }
                    )
                ]
            )
            chunk_ids.append(point_id)
        
        return chunk_ids
    
    def add_training_qa(self, db: Session, intent_id: int, question_text: str, answer_text: str):
        """
        Add training Q&A pair v√†o Qdrant
        
        Ch·ªâ embed question, kh√¥ng embed answer:
        - Answer stored ·ªü DB, retrieve khi match found
        - Question d√πng ƒë·ªÉ search/match
        - Ti·∫øt ki·ªám storage, tƒÉng search speed
        
        Args:
            question_id: Primary key c·ªßa training Q&A
            intent_id: Intent n√†y thu·ªôc intent n√†o
            question_text: Question ƒë·ªÉ embed
            answer_text: Answer (l∆∞u ·ªü DB, kh√¥ng embed)
        
        Returns:
            embedding_id: Qdrant point ID
        """
        new_qa = TrainingQuestionAnswer(
            question=question_text,
            answer=answer_text,
            intent_id=1,
            created_by=1
        )
        db.add(new_qa)
        db.commit()
        db.refresh(new_qa)
        # Embed question text
        embedding = self.embeddings.embed_query(question_text)
        point_id = str(uuid.uuid4())
        
        # Upsert v√†o training_qa collection
        # Metadata:
        # - question_id: Link v·ªÅ DB
        # - intent_id: ƒê·ªÉ track intent stats
        # - question_text: L∆∞u original text (optional, space saving)
        # - answer_text: L∆∞u answer (retrieve khi match)
        self.qdrant_client.upsert(
            collection_name=self.training_qa_collection,
            points=[
                PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        "question_id": new_qa.question_id,
                        "intent_id": intent_id,
                        "question_text": question_text,
                        "answer_text": answer_text,
                        "type": "training_qa"
                    }
                )
            ]
        )
        
        return {
            "postgre_question_id": new_qa.question_id,
            "qdrant_question_id": point_id
        }
    
    def search_documents(self, query: str, top_k: int = 5):
        """
        Search documents (Fallback)
        
        Fallback path: T√¨m document chunks khi training Q&A kh√¥ng match
        - Query ‚Üí Embed ‚Üí Search documents collection
        - Return top_k chunks
        - LLM s·∫Ω synthesize answer t·ª´ chunks
        
        Args:
            query: User question
            top_k: S·ªë chunks (lower score ‚Üí fallback)
        
        Returns:
            List of document chunks
        """
        
        query_embedding = self.embeddings.embed_query(query)
        
        results = self.qdrant_client.search(
            collection_name=self.documents_collection,
            query_vector=query_embedding,
            limit=top_k
        )
        
        return results
    
    def search_training_qa(self, query: str, top_k: int = 5):
        """
        Search training Q&A (Priority 1)
        
        Fast path: T√¨m pre-approved answers
        - Query ‚Üí Embed ‚Üí Search training_qa collection
        - Return top_k matches
        - filter score > 0.8
        
        Args:
            query: User question
            top_k: S·ªë results (default 5)
        
        Returns:
            List of search results with scores
        """
        
        query_embedding = self.embeddings.embed_query(query)
        
        results = self.qdrant_client.search(
            collection_name=self.training_qa_collection,
            query_vector=query_embedding,
            limit=top_k
        )
        
        return results
    def hybrid_search(self, query: str):
        """
        Hybrid RAG Search Strategy
        
        PRIORITY SYSTEM (Cascade):
        1. TIER 1 - Training Q&A (score > 0.8)
           - Highest confidence, direct answer
           - No LLM needed, fast response
           
        2. TIER 2 - Training Q&A (0.7 < score <= 0.8)
           - Good match but not perfect
           - Use as primary answer + add document context
           
        3. TIER 3 - Document Search + LLM Generation
           - No training Q&A match
           - Search documents, LLM synthesize
           - Lower confidence, show sources
           
        4. TIER 4 - Fallback
           - Nothing found
           - Suggest live chat with officer
        
        Returns:
            {
                "response": str,
                "response_source": "training_qa" | "document" | "fallback",
                "confidence": float,
                "top_match": obj,
                "intent_id": int,
                "sources": list
            }
        """
        
        # STEP 1: Search training Q&A
        qa_results = self.search_training_qa(query, top_k=3)
        
        # TIER 1: Perfect match (score > 0.8)
        if qa_results and qa_results[0].score > 0.8:
            top_match = qa_results[0]
            return {
                "response_official_answer": top_match.payload.get("answer_text"),
                "response_source": "training_qa",
                "confidence": top_match.score,
                "top_match": top_match,
                "intent_id": top_match.payload.get("intent_id"),
                "question_id": top_match.payload.get("question_id"),
                "sources": []
            }
        
        
        # TIER 2: No training Q&A match, try documents
        doc_results = self.search_documents(query, top_k=5)
        if doc_results and len(doc_results) > 0: 
            return {
                    "response": doc_results,
                    "response_source": "document",
                    "confidence": doc_results[0].score,
                    "top_match": doc_results[0],
                    "intent_id": doc_results[0].payload.get("intent_id"),
                    "sources": [r.payload.get("document_id") for r in doc_results]
                }
        else:
            return {
                "response": doc_results,
                "response_source": "document",
                "confidence": 0.0,
                "top_match": None,
                "intent_id": 0,
                "sources": []
            }
        
    def _get_user_personality_and_academics(self, user_id: int, db: Session) -> Dict[str, Any]:
        out = {
            "personality_summary": None,
            "riasec": None,
            "academic_summary": None,
            "gpa": None,
            "subjects": {}
        }

        # --- RIASEC result ---
        ri = (
            db.query(RiasecResult)
            .filter(RiasecResult.customer_id == user_id)
            .order_by(RiasecResult.result_id.desc())
            .first()
        )

        if ri:
            out["riasec"] = {
                "R": ri.score_realistic,
                "I": ri.score_investigative,
                "A": ri.score_artistic,
                "S": ri.score_social,
                "E": ri.score_enterprising,
                "C": ri.score_conventional,
            }
            # `result` field = summary c·ªßa b·∫°n
            out["personality_summary"] = ri.result or self._riasec_to_summary(out["riasec"])

        # --- Academic scores ---
        score = (
            db.query(AcademicScore)
            .filter(AcademicScore.customer_id == user_id)
            .first()
        )

        if score:
            subj_map = {
            "math": score.math,
            "literature": score.literature,
            "english": score.english,
            "physics": score.physics,
            "chemistry": score.chemistry,
            "biology": score.biology,
            "history": score.history,
            "geography": score.geography,
        }

            # simple GPA = average score
            valid_scores = [v for v in subj_map.values() if v is not None]
            gpa = round(sum(valid_scores) / len(valid_scores), 2)

            out["subjects"] = subj_map
            out["gpa"] = gpa
            out["academic_summary"] = (
                f"GPA x·∫•p x·ªâ {gpa}. C√°c m√¥n: " +
                ", ".join([f"{k}: {v}" for k, v in subj_map.items()])
            )
            print(out["academic_summary"])
        return out

    def _riasec_to_summary(self, ri_map: Dict[str,int]) -> str:
        # very small helper - b·∫°n c√≥ th·ªÉ m·ªü r·ªông
        order = sorted(ri_map.items(), key=lambda x: -x[1])
        top = order[0][0] if order else None
        return f"∆Øu th·∫ø RIASEC: {', '.join([f'{k}={v}' for k,v in ri_map.items()])}. Ch√≠nh: {top}."

    def _get_all_majors_from_db(self, db: Session, limit: int = 200) -> List[Dict[str,Any]]:
        """
        L·∫•y danh s√°ch majors
        """
        rows = db.query(Major).order_by(Major.major_name).limit(limit).all()
        majors = []
        for r in rows:
            majors.append({
                "major_id": r.major_id,
                "major_name": r.major_name,
            })
        return majors

    
    

    

langchain_service = TrainingService()
