from datetime import datetime
from typing import Dict, List
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters  import RecursiveCharacterTextSplitter
from langchain_classic.memory import ConversationBufferMemory
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import os
import uuid
import asyncio
from sqlalchemy.orm import Session
from app.models.entities import ChatInteraction, ChatSession, ParticipateChatSession, TrainingQuestionAnswer
from app.models.database import SessionLocal
from sqlalchemy.exc import SQLAlchemyError
from app.services.memory_service import MemoryManager

memory_service = MemoryManager()

class TrainingService:
    def __init__(self):
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        self.llm = GoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=self.gemini_api_key,
            temperature=0.7
        )
        self.embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/gemini-embedding-001",
                    google_api_key=self.gemini_api_key
        )
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", 6333))
        )
        self.training_qa_collection = "training_qa"
        self.documents_collection = "knowledge_base_documents"
        self._init_collections()

    def _init_collections(self):
        try:
            self.qdrant_client.create_collection(
                collection_name=self.training_qa_collection,
                vectors_config=VectorParams(size=3072, distance=Distance.COSINE)
            )
        except:
            pass
            
        try:
            self.qdrant_client.create_collection(
                collection_name=self.documents_collection,
                vectors_config=VectorParams(size=3072, distance=Distance.COSINE)
            )
        except:
            pass

    def create_chat_session(self, user_id: int, session_type: str = "chatbot") -> int:
        """
        Táº¡o chat session má»›i
        
        Args:
            user_id: ID cá»§a user
            session_type: "chatbot" hoáº·c "live"
        
        Returns:
            session_id: ID cá»§a session vá»«a táº¡o
        """
        db = SessionLocal()
        try:
            session = ChatSession(
                session_type=session_type,
                start_time=datetime.datetime.now()
            )
            db.add(session)
            db.flush()
            
            # Add user vÃ o participate table
            participate = ParticipateChatSession(
                user_id=user_id,
                session_id=session.chat_session_id
            )
            db.add(participate)
            db.commit()
            
            return session.chat_session_id
        except SQLAlchemyError as e:
            db.rollback()
            print(f"Error creating session: {e}")
            raise
        finally:
            db.close()

    def get_session_history(self, session_id: int, limit: int = 50) -> List[Dict]:
        """
        Láº¥y lá»‹ch sá»­ chat cá»§a session
        
        Returns:
            List of messages [{message_text, timestamp, is_from_bot}, ...]
        """
        db = SessionLocal()
        try:
            interactions = db.query(ChatInteraction).filter(
                ChatInteraction.session_id == session_id
            ).order_by(
                ChatInteraction.timestamp.asc()
            ).limit(limit).all()
            
            return [
                {
                    "message_text": i.message_text,
                    "timestamp": i.timestamp.isoformat() if i.timestamp else None,
                    "is_from_bot": i.is_from_bot,
                    "rating": i.rating
                }
                for i in interactions
            ]
        finally:
            db.close()
    
    def get_user_sessions(self, user_id: int) -> List[Dict]:
        """
        Láº¥y táº¥t cáº£ sessions cá»§a user (Ä‘á»ƒ hiá»ƒn thá»‹ recent chats)
        
        Returns:
            List of sessions vá»›i preview message cuá»‘i cÃ¹ng
        """
        db = SessionLocal()
        try:
            sessions = db.query(ChatSession).join(
                ParticipateChatSession
            ).filter(
                ParticipateChatSession.user_id == user_id
            ).order_by(
                ChatSession.start_time.desc()
            ).all()
            
            result = []
            for session in sessions:
                # Láº¥y message cuá»‘i cÃ¹ng lÃ m preview
                last_msg = db.query(ChatInteraction).filter(
                    ChatInteraction.session_id == session.chat_session_id
                ).order_by(
                    ChatInteraction.timestamp.desc()
                ).first()
                
                result.append({
                    "session_id": session.chat_session_id,
                    "session_type": session.session_type,
                    "start_time": session.start_time.isoformat() if session.start_time else None,
                    "last_message_preview": last_msg.message_text[:50] + "..." if last_msg else "",
                    "last_message_time": last_msg.timestamp.isoformat() if last_msg and last_msg.timestamp else None
                })
            
            return result
        finally:
            db.close()

    # ---------------------------
    # Query enrichment: dÃ¹ng chat_history + last bot question Ä‘á»ƒ build a full query
    # ---------------------------
    async def enrich_query(self, session_id: str, user_message: str) -> str:
        memory = memory_service.get_memory(session_id)
        mem_vars = memory.load_memory_variables({})
        chat_history = mem_vars.get("chat_history", "")

        prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ giÃºp chuyá»ƒn cÃ¡c cÃ¢u tráº£ lá»i cá»§a ngÆ°á»i dÃ¹ng thÃ nh cÃ¡c truy váº¥n tÃ¬m kiáº¿m Ä‘áº§y Ä‘á»§ cho chatbot RAG tÆ° váº¥n tuyá»ƒn sinh.

        Cuá»™c há»™i thoáº¡i gáº§n Ä‘Ã¢y (theo thá»© tá»± tá»« cÅ© Ä‘áº¿n má»›i):
        {chat_history}

        Pháº£n há»“i má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng: "{user_message}"

        Nhiá»‡m vá»¥: Dá»±a trÃªn "cuá»™c há»™i thoáº¡i gáº§n Ä‘Ã¢y" vÃ  "pháº£n há»“i má»›i nháº¥t cá»§a ngÆ°á»i dÃ¹ng", báº¡n hÃ£y Ä‘áº£m báº£o táº¡o ra **má»™t cÃ¢u truy váº¥n tÃ¬m kiáº¿m**, ngáº¯n gá»n, rÃµ rÃ ng, cá»¥ thá»ƒ (báº±ng tiáº¿ng Viá»‡t), thá»ƒ hiá»‡n Ä‘Ãºng Ã½ Ä‘á»‹nh cá»§a ngÆ°á»i dÃ¹ng Ä‘á»ƒ gá»­i cho chatbot rag tÆ° váº¥n Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ hiá»ƒu yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng. "Chá»‰ táº¡o truy váº¥n náº¿u pháº£n há»“i cá»§a ngÆ°á»i dÃ¹ng lÃ  pháº§n tiáº¿p ná»‘i hoáº·c lÃ m rÃµ ná»™i dung trong há»™i thoáº¡i trÆ°á»›c Ä‘Ã³.", náº¿u pháº£n há»“i cá»§a ngÆ°á»i dÃ¹ng khÃ´ng tráº£ lá»i hoáº·c khÃ´ng liÃªn quan cho cuá»™c há»™i thoáº¡i gáº§n Ä‘Ã¢y thÃ¬ hÃ£y tráº£ vá» chuá»—i rá»—ng.

        Chá»‰ tráº£ vá» **má»™t dÃ²ng truy váº¥n duy nháº¥t** (khÃ´ng thÃªm ná»™i dung khÃ¡c).  
        VÃ­ dá»¥:
        - "ThÃ´ng tin vá» ngÃ nh CÃ´ng nghá»‡ ThÃ´ng tin táº¡i trÆ°á»ng XYZ"  
        - "Há»c phÃ­ ngÃ nh CNTT há»‡ chÃ­nh quy nÄƒm 2025 táº¡i trÆ°á»ng XYZ"
        """
        # assume async predict exists
        enriched = await self.llm.ainvoke(prompt)
        # fallback: if empty use original
        enriched_txt = (enriched or "").strip().splitlines()[0] if enriched else user_message
        return enriched_txt   

    # ---------------------------
    # LLM relevance check: ensure enriched_query actually matches the training QA
    # ---------------------------
    async def llm_relevance_check(self, enriched_query: str, matched_question: str, answer: str) -> bool:
        prompt = f"""
        Báº¡n lÃ  chuyÃªn gia Ä‘Ã¡nh giÃ¡ giá»¯a cÃ¢u há»i tÃ¬m kiáº¿m, cÃ¢u há»i trong cÆ¡ sá»Ÿ dá»¯ liá»‡u vÃ  cÃ¢u tráº£ lá»i cho 1 há»‡ thá»‘ng chat RAG tuyá»ƒn sinh, hÃ£y suy luáº­n. 

        CÃ¢u há»i tÃ¬m kiáº¿m (Ä‘Ã£ chuáº©n hÃ³a): "{enriched_query}"
        CÃ¢u há»i DB: "{matched_question}"
        CÃ¢u tráº£ lá»i chÃ­nh thá»©c: "{answer}"

        HÃ£y tráº£ lá»i duy nháº¥t chá»‰ má»™t tá»«: "true" náº¿u cÃ¢u há»i DB phÃ¹ há»£p vÃ  tráº£ lá»i Ä‘Ã³ há»£p lÃ½ cho truy váº¥n tÃ¬m kiáº¿m; "false" náº¿u chá»‰ trÃ¹ng tá»« khÃ³a hoáº·c khÃ´ng phÃ¹ há»£p.
        """
        res = await self.llm.ainvoke(prompt)
        if not res:
            return False
        r = res.strip().lower()
        return ("Ä‘Ãºng" in r) or ("true" in r) or (r.startswith("Ä‘Ãºng")) or (r.startswith("true"))

    async def load_session_history_to_memory(self, session_id: int, db: Session):
        memory = memory_service.get_memory(session_id)

        # Láº¥y lá»‹ch sá»­ chat theo thá»© tá»± thá»i gian
        interactions = (
            db.query(ChatInteraction)
            .filter(ChatInteraction.session_id == session_id)
            .order_by(ChatInteraction.timestamp.asc())
            .all()
        )

        last_user_msg = None
        for inter in interactions:
            if not inter.is_from_bot:
                # user message
                last_user_msg = inter.message_text
            else:
                # bot message -> káº¿t há»£p vá»›i user message trÆ°á»›c Ä‘Ã³ (náº¿u cÃ³)
                memory.save_context(
                    {"input": last_user_msg or ""},
                    {"output": inter.message_text}
                )
                last_user_msg = None

        # Náº¿u cuá»‘i cÃ¹ng lÃ  tin nháº¯n user chÆ°a Ä‘Æ°á»£c pháº£n há»“i
        if last_user_msg:
            memory.save_context({"input": last_user_msg}, {"output": ""})

    async def stream_response_from_context(self, query: str, context: str, session_id: int = 1, user_id: int = 1):
        db = SessionLocal()
        
        try:
            # ðŸ§© 1. LÆ°u tin nháº¯n ngÆ°á»i dÃ¹ng
            user_msg = ChatInteraction(
                message_text=query,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=False,
                sender_id=user_id,
                session_id=session_id
            )
            db.add(user_msg)
            db.flush()  # flush Ä‘á»ƒ láº¥y ID náº¿u cáº§n liÃªn káº¿t sau
        
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")
            """Stream pháº£n há»“i tá»« Gemini, tá»«ng chunk má»™t."""
            prompt = f"""Báº¡n lÃ  má»™t chatbot tÆ° váº¥n tuyá»ƒn sinh chuyÃªn nghiá»‡p cá»§a trÆ°á»ng XYZ
            ÄÃ¢y lÃ  Ä‘oáº¡n há»™i thoáº¡i trÆ°á»›c: 
            {chat_history}
            === THÃ”NG TIN THAM KHáº¢O ===
            {context}
            === CÃ‚U Há»ŽI ===
            {query}
            === HÆ¯á»šNG DáºªN ===
            - Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
            - ThÃ¢n thiá»‡n, chuyÃªn nghiá»‡p
            - Dá»±a vÃ o thÃ´ng tin tham kháº£o trÃªn Ä‘Æ°á»£c cung cáº¥p
            - Báº¡n lÃ  chatbot tÆ° váº¥n tuyá»ƒn sinh cá»§a trÆ°á»ng xyz, náº¿u thÃ´ng tin cÃ¢u há»i yÃªu cÃ¢u tÃªn 1 trÆ°á»ng khÃ¡c thÃ¬ hÃ£y nÃ³i rÃµ ra lÃ  khÃ´ng tÃ¬m tháº¥y thÃ´ng tin
            - Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i rÃµ vÃ  gá»£i Ã½ liÃªn há»‡ trá»±c tiáº¿p nhÃ¢n viÃªn tÆ° váº¥n
            - KhÃ´ng bá»‹a thÃªm thÃ´ng tin ngoÃ i context
            - Náº¿u cÃ¢u há»i chá»‰ lÃ  chÃ o há»i, há»i thá»i tiáº¿t, hoáº·c cÃ¡c cÃ¢u xÃ£ giao, hÃ£y tráº£ lá»i báº±ng lá»i chÃ o thÃ¢n thiá»‡n, giá»›i thiá»‡u vá» báº£n thÃ¢n chatbot, KHÃ”NG kÃ©o thÃªm thÃ´ng tin chi tiáº¿t trong context.
            - CÃ³ thá»ƒ **diá»…n Ä‘áº¡t láº¡i cÃ¢u há»i hoáº·c thÃ´ng tin** má»™t cÃ¡ch nháº¹ nhÃ ng, tá»± nhiÃªn Ä‘á»ƒ ngÆ°á»i dÃ¹ng dá»… hiá»ƒu hÆ¡n, **nhÆ°ng tuyá»‡t Ä‘á»‘i khÃ´ng thay Ä‘á»•i Ã½ nghÄ©a hay thÃªm dá»¯ kiá»‡n má»›i.**
            - Khi cÃ³ thá»ƒ, hÃ£y **giáº£i thÃ­ch thÃªm bá»‘i cáº£nh hoáº·c gá»£i Ã½ bÆ°á»›c tiáº¿p theo**, vÃ­ dá»¥:  
                â€œBáº¡n muá»‘n mÃ¬nh gá»­i danh sÃ¡ch ngÃ nh Ä‘Ã o táº¡o kÃ¨m chuyÃªn ngÃ nh chi tiáº¿t khÃ´ng?â€  
                hoáº·c  
                â€œNáº¿u báº¡n quan tÃ¢m há»c bá»•ng, mÃ¬nh cÃ³ thá»ƒ nÃ³i rÃµ cÃ¡c loáº¡i há»c bá»•ng hiá»‡n cÃ³ nhÃ©!â€
            """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                yield chunk
                full_response += chunk
                await asyncio.sleep(0)  # NhÆ°á»ng event loop
            memory.save_context({"input": query}, {"output": full_response})  
            
            # === ðŸ”¥ LÆ°u bot response vÃ o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)

            # ðŸ§© 5. Commit 1 láº§n duy nháº¥t
            db.commit()
            print(f"ðŸ’¾ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close()
    async def stream_response_from_qa(self, query: str, context: str, session_id: int = 1, user_id: int = 1):
      
        memory = memory_service.get_memory(session_id)
        mem_vars = memory.load_memory_variables({})
        chat_history = mem_vars.get("chat_history", "")
        prompt = f"""
        Báº¡n lÃ  chatbot tÆ° váº¥n tuyá»ƒn sinh cá»§a trÆ°á»ng XYZ.
        ÄÃ¢y lÃ  Ä‘oáº¡n há»™i thoáº¡i trÆ°á»›c: 
        {chat_history}
        === CÃ‚U TRáº¢ Lá»œI CHÃNH THá»¨C ===
        {context}

        === CÃ‚U Há»ŽI NGÆ¯á»œI DÃ™NG ===
        {query}

        === HÆ¯á»šNG DáºªN TRáº¢ Lá»œI ===
        - HÃ£y Ä‘á»c ká»¹ pháº§n NGá»® Cáº¢NH LIÃŠN QUAN, nhÆ°ng **chá»‰ sá»­ dá»¥ng nÃ³ náº¿u tháº­t sá»± cÃ³ ná»™i dung trÃ¹ng khá»›p hoáº·c phÃ¹ há»£p vá»›i cÃ¢u há»i ngÆ°á»i dÃ¹ng.**
        - Náº¿u pháº§n CÃ‚U TRáº¢ Lá»œI CHÃNH THá»¨C khÃ´ng liÃªn quan rÃµ rÃ ng Ä‘áº¿n cÃ¢u há»i, **Ä‘á»«ng cá»‘ tráº£ lá»i theo context** mÃ  hÃ£y nÃ³i:
        â€œHiá»‡n chÆ°a cÃ³ thÃ´ng tin chÃ­nh xÃ¡c cho cÃ¢u há»i nÃ y. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ chi tiáº¿t hÆ¡n Ä‘Æ°á»£c khÃ´ng?â€ 
        - Náº¿u pháº§n tráº£ lá»i chÃ­nh thá»©c khÃ´ng phÃ¹ há»£p vá»›i cÃ¢u há»i, hÃ£y nÃ³i â€œHiá»‡n chÆ°a cÃ³ thÃ´ng tin cho cÃ¢u há»i nÃ y. Vui lÃ²ng liÃªn há»‡ chuyÃªn viÃªn tÆ° váº¥n.â€
        - Báº¡n lÃ  chatbot tÆ° váº¥n tuyá»ƒn sinh cá»§a trÆ°á»ng xyz, nhá»› kiá»ƒm tra kÄ© rÃµ rÃ ng cÃ¢u há»i, náº¿u thÃ´ng tin cÃ¢u há»i yÃªu cÃ¢u tÃªn 1 trÆ°á»ng khÃ¡c thÃ¬ hÃ£y nÃ³i rÃµ ra lÃ  khÃ´ng tÃ¬m tháº¥y thÃ´ng tin
        - Náº¿u cÃ¢u há»i chá»‰ lÃ  chÃ o há»i, há»i thá»i tiáº¿t, hoáº·c cÃ¡c cÃ¢u xÃ£ giao, hÃ£y tráº£ lá»i báº±ng lá»i chÃ o thÃ¢n thiá»‡n, giá»›i thiá»‡u vá» báº£n thÃ¢n chatbot, KHÃ”NG kÃ©o thÃªm thÃ´ng tin chi tiáº¿t trong context.
        - Náº¿u cÃ¢u há»i quÃ¡ mÆ¡ há»“, hÃ£y há»i láº¡i Ä‘á»ƒ rÃµ hÆ¡n vÃ  chi tiáº¿t hÆ¡n vá» cÃ¢u há»i
        - CÃ³ thá»ƒ **diá»…n Ä‘áº¡t láº¡i cÃ¢u há»i hoáº·c thÃ´ng tin** má»™t cÃ¡ch nháº¹ nhÃ ng, tá»± nhiÃªn Ä‘á»ƒ ngÆ°á»i dÃ¹ng dá»… hiá»ƒu hÆ¡n, **nhÆ°ng tuyá»‡t Ä‘á»‘i khÃ´ng thay Ä‘á»•i Ã½ nghÄ©a hay thÃªm dá»¯ kiá»‡n má»›i.**
        - Khi cÃ³ thá»ƒ, hÃ£y **giáº£i thÃ­ch thÃªm bá»‘i cáº£nh hoáº·c gá»£i Ã½ bÆ°á»›c tiáº¿p theo**, vÃ­ dá»¥:  
            â€œBáº¡n muá»‘n mÃ¬nh gá»­i danh sÃ¡ch ngÃ nh Ä‘Ã o táº¡o kÃ¨m chuyÃªn ngÃ nh chi tiáº¿t khÃ´ng?â€  
            hoáº·c  
            â€œNáº¿u báº¡n quan tÃ¢m há»c bá»•ng, mÃ¬nh cÃ³ thá»ƒ nÃ³i rÃµ cÃ¡c loáº¡i há»c bá»•ng hiá»‡n cÃ³ nhÃ©!â€
        """
        full_response = ""
        async for chunk in self.llm.astream(prompt):
            yield chunk
            full_response += chunk 
            await asyncio.sleep(0)  # NhÆ°á»ng event loop

        memory.save_context({"input": query}, {"output": full_response})  
        print("Saved to memory. Current messages:", len(self.memory.chat_memory.messages)) 
    
    def add_document(self, document_id: int, content: str, metadata: dict = None):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # Size optimal cho Vietnamese
            chunk_overlap=200     # Overlap to preserve context
        )
        chunks = text_splitter.split_text(content)
        
        chunk_ids = []
        for i, chunk in enumerate(chunks):
            # Embed chunk
            embedding = self.embeddings.embed_query(chunk)
            point_id = str(uuid.uuid4())
            
            # Upsert to Qdrant
            self.qdrant_client.upsert(
                collection_name="knowledge_base_documents",
                points=[
                    PointStruct(
                        id=point_id,
                        vector=embedding,
                        payload={
                            "document_id": document_id,
                            "chunk_index": i,
                            "chunk_text": chunk,
                            "metadata": metadata or {},
                            "type": "document"
                        }
                    )
                ]
            )
            chunk_ids.append(point_id)
        
        return chunk_ids
    def add_training_qa(self, db: Session, intent_id: int, question_text: str, answer_text: str):
        """
        Add training Q&A pair vÃ o Qdrant
        
        Chá»‰ embed question, khÃ´ng embed answer:
        - Answer stored á»Ÿ DB, retrieve khi match found
        - Question dÃ¹ng Ä‘á»ƒ search/match
        - Tiáº¿t kiá»‡m storage, tÄƒng search speed
        
        Args:
            question_id: Primary key cá»§a training Q&A
            intent_id: Intent nÃ y thuá»™c intent nÃ o
            question_text: Question Ä‘á»ƒ embed
            answer_text: Answer (lÆ°u á»Ÿ DB, khÃ´ng embed)
        
        Returns:
            embedding_id: Qdrant point ID
        """
        new_qa = TrainingQuestionAnswer(
            question=question_text,
            answer=answer_text,
            intent_id=1,
            created_by=1
        )
        db.add(new_qa)
        db.commit()
        db.refresh(new_qa)
        # Embed question text
        embedding = self.embeddings.embed_query(question_text)
        point_id = str(uuid.uuid4())
        
        # Upsert vÃ o training_qa collection
        # Metadata:
        # - question_id: Link vá» DB
        # - intent_id: Äá»ƒ track intent stats
        # - question_text: LÆ°u original text (optional, space saving)
        # - answer_text: LÆ°u answer (retrieve khi match)
        self.qdrant_client.upsert(
            collection_name=self.training_qa_collection,
            points=[
                PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        "question_id": new_qa.question_id,
                        "intent_id": 1,
                        "question_text": question_text,
                        "answer_text": answer_text,
                        "type": "training_qa"
                    }
                )
            ]
        )
        
        return {
            "postgre_question_id": new_qa.question_id,
            "qdrant_question_id": point_id
        }
    def search_documents(self, query: str, top_k: int = 5):
        """
        Search documents (Fallback)
        
        Fallback path: TÃ¬m document chunks khi training Q&A khÃ´ng match
        - Query â†’ Embed â†’ Search documents collection
        - Return top_k chunks
        - LLM sáº½ synthesize answer tá»« chunks
        
        Args:
            query: User question
            top_k: Sá»‘ chunks (lower score â†’ fallback)
        
        Returns:
            List of document chunks
        """
        
        query_embedding = self.embeddings.embed_query(query)
        
        results = self.qdrant_client.search(
            collection_name=self.documents_collection,
            query_vector=query_embedding,
            limit=top_k
        )
        
        return results
    
    def search_training_qa(self, query: str, top_k: int = 5):
        """
        Search training Q&A (Priority 1)
        
        Fast path: TÃ¬m pre-approved answers
        - Query â†’ Embed â†’ Search training_qa collection
        - Return top_k matches
        - filter score > 0.8
        
        Args:
            query: User question
            top_k: Sá»‘ results (default 5)
        
        Returns:
            List of search results with scores
        """
        
        query_embedding = self.embeddings.embed_query(query)
        
        results = self.qdrant_client.search(
            collection_name=self.training_qa_collection,
            query_vector=query_embedding,
            limit=top_k
        )
        
        return results
    def hybrid_search(self, query: str):
        """
        Hybrid RAG Search Strategy
        
        PRIORITY SYSTEM (Cascade):
        1. TIER 1 - Training Q&A (score > 0.8)
           - Highest confidence, direct answer
           - No LLM needed, fast response
           
        2. TIER 2 - Training Q&A (0.7 < score <= 0.8)
           - Good match but not perfect
           - Use as primary answer + add document context
           
        3. TIER 3 - Document Search + LLM Generation
           - No training Q&A match
           - Search documents, LLM synthesize
           - Lower confidence, show sources
           
        4. TIER 4 - Fallback
           - Nothing found
           - Suggest live chat with officer
        
        Returns:
            {
                "response": str,
                "response_source": "training_qa" | "document" | "fallback",
                "confidence": float,
                "top_match": obj,
                "intent_id": int,
                "sources": list
            }
        """
        
        # STEP 1: Search training Q&A
        qa_results = self.search_training_qa(query, top_k=3)
        
        # TIER 1: Perfect match (score > 0.8)
        if qa_results and qa_results[0].score > 0.8:
            top_match = qa_results[0]
            return {
                "response_official_answer": top_match.payload.get("answer_text"),
                "response_source": "training_qa",
                "confidence": top_match.score,
                "top_match": top_match,
                "intent_id": top_match.payload.get("intent_id"),
                "question_id": top_match.payload.get("question_id"),
                "sources": []
            }
        
        
        # TIER 2: No training Q&A match, try documents
        else: doc_results = self.search_documents(query, top_k=5)
        return {
                "response": doc_results,
                "response_source": "document",
                "confidence": doc_results[0].score,
                "top_match": None,
                "intent_id": None,
                "sources": [r.payload.get("document_id") for r in doc_results]
            }
        
      

    

langchain_service = TrainingService()
