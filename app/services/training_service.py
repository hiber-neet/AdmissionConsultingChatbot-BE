from datetime import datetime
from typing import Any, Dict, List, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters  import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models
from qdrant_client.models import Distance, VectorParams, PointStruct
import os
import uuid
import asyncio
from sqlalchemy.orm import Session
from app.models import schemas
from app.models.entities import AcademicScore, ChatInteraction, ChatSession, DocumentChunk, FaqStatistics, KnowledgeBaseDocument, Major, ParticipateChatSession, RiasecResult, TrainingQuestionAnswer
from app.models.database import SessionLocal
from sqlalchemy.exc import SQLAlchemyError
from app.services.memory_service import MemoryManager
from app.utils.document_processor import DocumentProcessor

memory_service = MemoryManager()

class TrainingService:
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(
            model="gpt-4.1-mini",
            api_key=self.openai_api_key,
            temperature=0.7
        )
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=self.openai_api_key
        )
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", 6333))
        )
        self.training_qa_collection = "training_qa"
        self.documents_collection = "knowledge_base_documents"
        self._init_collections()

    def _init_collections(self):
        try:
            self.qdrant_client.create_collection(
                collection_name=self.training_qa_collection,
                vectors_config=VectorParams(size=3072, distance=Distance.COSINE)
            )
        except:
            pass
            
        try:
            self.qdrant_client.create_collection(
                collection_name=self.documents_collection,
                vectors_config=VectorParams(size=3072, distance=Distance.COSINE)
            )
        except:
            pass

    def create_chat_session(self, user_id: int, session_type: str = "chatbot") -> int:
        """
        T·∫°o chat session m·ªõi
        
        Args:
            user_id: ID c·ªßa user
            session_type: "chatbot" ho·∫∑c "live"
        
        Returns:
            session_id: ID c·ªßa session v·ª´a t·∫°o
        """
        
        db = SessionLocal()
        if not user_id:
            session = ChatSession(
                session_type=session_type,
                start_time=datetime.now()
            )
            db.add(session)
            db.flush()
            db.commit()
            return session.chat_session_id
        try:
            session = ChatSession(
                session_type=session_type,
                start_time=datetime.now()
            )
            db.add(session)
            db.flush()
            
            # Add user v√†o participate table
            participate = ParticipateChatSession(
                user_id=user_id,
                session_id=session.chat_session_id
            )
            db.add(participate)
            db.commit()
            
            return session.chat_session_id
        except SQLAlchemyError as e:
            db.rollback()
            print(f"Error creating session: {e}")
            raise
        finally:
            db.close()

    def get_session_history(self, session_id: int, limit: int = 50) -> List[Dict]:
        """
        L·∫•y l·ªãch s·ª≠ chat c·ªßa session
        
        Returns:
            List of messages [{message_text, timestamp, is_from_bot}, ...]
        """
        db = SessionLocal()
        try:
            interactions = db.query(ChatInteraction).filter(
                ChatInteraction.session_id == session_id
            ).order_by(
                ChatInteraction.timestamp.asc()
            ).limit(limit).all()
            
            return [
                {
                    "message_text": i.message_text,
                    "timestamp": i.timestamp.isoformat() if i.timestamp else None,
                    "is_from_bot": i.is_from_bot,
                    "rating": i.rating
                }
                for i in interactions
            ]
        finally:
            db.close()
    
    def get_user_sessions(self, user_id: int) -> List[Dict]:
        """
        L·∫•y t·∫•t c·∫£ sessions c·ªßa user (ƒë·ªÉ hi·ªÉn th·ªã recent chats)
        
        Returns:
            List of sessions v·ªõi preview message cu·ªëi c√πng
        """
        db = SessionLocal()
        try:
            sessions = db.query(ChatSession).join(
                ParticipateChatSession
            ).filter(
                ParticipateChatSession.user_id == user_id
            ).order_by(
                ChatSession.start_time.desc()
            ).all()
            
            result = []
            for session in sessions:
                # L·∫•y message cu·ªëi c√πng l√†m preview
                last_msg = db.query(ChatInteraction).filter(
                    ChatInteraction.session_id == session.chat_session_id
                ).order_by(
                    ChatInteraction.timestamp.desc()
                ).first()
                
                result.append({
                    "session_id": session.chat_session_id,
                    "session_type": session.session_type,
                    "start_time": session.start_time.isoformat() if session.start_time else None,
                    "last_message_preview": last_msg.message_text[:50] + "..." if last_msg else "",
                    "last_message_time": last_msg.timestamp.isoformat() if last_msg and last_msg.timestamp else None
                })
            
            return result
        finally:
            db.close()

    def delete_chat_session(self, session_id: int, user_id: Optional[int] = None) -> bool:
        """
        X√≥a 1 session chat:
        - N·∫øu c√≥ user_id: ch·ªâ x√≥a session thu·ªôc v·ªÅ user ƒë√≥
        - N·∫øu kh√¥ng c√≥ user_id: x√≥a theo session_id (guest session)

        Tr·∫£ v·ªÅ:
            True  n·∫øu x√≥a ƒë∆∞·ª£c
            False n·∫øu kh√¥ng t√¨m th·∫•y session
        """
        db = SessionLocal()
        try:
            query = db.query(ChatSession)

            # N·∫øu c√≥ user_id th√¨ check session thu·ªôc user ƒë√≥
            if user_id:
                query = query.join(ParticipateChatSession).filter(
                    ParticipateChatSession.user_id == user_id
                )

            session = query.filter(
                ChatSession.chat_session_id == session_id
            ).first()

            if not session:
                return False

            # Do ChatSession ƒë·ªãnh nghƒ©a cascade="all, delete-orphan"
            # n√™n x√≥a session s·∫Ω t·ª± x√≥a ChatInteraction & ParticipateChatSession li√™n quan
            db.delete(session)
            db.commit()
            return True

        except SQLAlchemyError as e:
            db.rollback()
            print(f"Error deleting session: {e}")
            raise
        finally:
            db.close()

    # ---------------------------
    # Query enrichment: d√πng chat_history + last bot question ƒë·ªÉ build a full query
    # ---------------------------
    async def enrich_query(self, session_id: str, user_message: str) -> str:
        memory = memory_service.get_memory(session_id)
        mem_vars = memory.load_memory_variables({})
        chat_history = mem_vars.get("chat_history", "")

        prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω gi√∫p chuy·ªÉn c√°c c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng th√†nh c√°c truy v·∫•n t√¨m ki·∫øm ƒë·∫ßy ƒë·ªß cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.

        Cu·ªôc h·ªôi tho·∫°i g·∫ßn ƒë√¢y (theo th·ª© t·ª± t·ª´ c≈© ƒë·∫øn m·ªõi):
        {chat_history}

        Ph·∫£n h·ªìi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng: "{user_message}"

        Nhi·ªám v·ª•: D·ª±a tr√™n "cu·ªôc h·ªôi tho·∫°i g·∫ßn ƒë√¢y" v√† "ph·∫£n h·ªìi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng", b·∫°n h√£y ƒë·∫£m b·∫£o t·∫°o ra **m·ªôt c√¢u truy v·∫•n t√¨m ki·∫øm**, r√µ r√†ng, c·ª• th·ªÉ (b·∫±ng ti·∫øng Vi·ªát), th·ªÉ hi·ªán ƒë√∫ng √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ g·ª≠i cho chatbot rag t∆∞ v·∫•n ƒë·ªÉ n√≥ c√≥ th·ªÉ hi·ªÉu y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng. "Ch·ªâ t·∫°o truy v·∫•n n·∫øu ph·∫£n h·ªìi c·ªßa ng∆∞·ªùi d√πng l√† ph·∫ßn ti·∫øp n·ªëi ho·∫∑c l√†m r√µ n·ªôi dung trong h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥.", n·∫øu ph·∫£n h·ªìi c·ªßa ng∆∞·ªùi d√πng kh√¥ng tr·∫£ l·ªùi ho·∫∑c kh√¥ng li√™n quan cho cu·ªôc h·ªôi tho·∫°i g·∫ßn ƒë√¢y th√¨ h√£y tr·∫£ v·ªÅ y nguy√™n ph·∫£n h·ªìi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng.

        """
        # assume async predict exists
        enriched = await self.llm.ainvoke(prompt)
        print("==== RAW RESPONSE ====")
        print(enriched.content)
        print("======================")
        # fallback: if empty use original
        enriched_txt = (enriched.content or "").strip().splitlines()[0] if enriched else user_message
        return enriched_txt   

    # ---------------------------
    # LLM relevance check: ensure enriched_query actually matches the training QA
    # ---------------------------
    async def llm_relevance_check(self, enriched_query: str, matched_question: str, answer: str) -> bool:
        prompt = f"""
        B·∫°n l√† chuy√™n gia ƒë√°nh gi√° gi·ªØa c√¢u h·ªèi t√¨m ki·∫øm, c√¢u h·ªèi trong c∆° s·ªü d·ªØ li·ªáu v√† c√¢u tr·∫£ l·ªùi cho 1 h·ªá th·ªëng chat RAG tuy·ªÉn sinh, h√£y suy lu·∫≠n. 

        C√¢u h·ªèi t√¨m ki·∫øm (ƒë√£ chu·∫©n h√≥a): "{enriched_query}"
        C√¢u h·ªèi DB: "{matched_question}"
        C√¢u tr·∫£ l·ªùi ch√≠nh th·ª©c: "{answer}"

        H√£y tr·∫£ l·ªùi duy nh·∫•t ch·ªâ m·ªôt t·ª´: "true" n·∫øu c√¢u h·ªèi DB ph√π h·ª£p v√† tr·∫£ l·ªùi ƒë√≥ h·ª£p l√Ω cho truy v·∫•n t√¨m ki·∫øm; "false" n·∫øu ch·ªâ tr√πng t·ª´ kh√≥a ho·∫∑c kh√¥ng ph√π h·ª£p.
        Ho·∫∑c c√≥ th·ªÉ tr·∫£ v·ªÅ "true" n·∫øu c√¢u h·ªèi t√¨m ki·∫øm ch·ªâ l√† l·ªùi ch√†o.
        """
        res = await self.llm.ainvoke(prompt)
        if not res.content:
            return False
        r = res.content.strip().lower()
        return ("ƒë√∫ng" in r) or ("true" in r) or (r.startswith("ƒë√∫ng")) or (r.startswith("true"))

    async def llm_document_recommendation_check(self, enriched_query: str, context: str) -> bool:
        prompt = f"""
        B·∫°n l√† h·ªá th·ªëng ki·ªÉm tra 2 t·∫ßng:
        - T·∫ßng 1 l√† h·ªá th·ªëng ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi ng∆∞·ªùi d√πng v√† n·ªôi dung trong Document Base (RAG) cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.
        - T·∫ßng 2 l√† h·ªá th·ªëng ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung t∆∞ v·∫•n ng√†nh h·ªçc hay t∆∞ v·∫•n cho c√° nh√¢n d·ª±a theo h·ªì s∆° c·ªßa h·ªçc sinh ho·∫∑c nh·ªØng c√¢u li√™n quan ƒë·∫øn RIASEC, h·ªçc b·∫°, GPA, s·ªü th√≠ch, nguy·ªán v·ªçng c√° nh√¢n; ho·∫∑c y√™u c·∫ßu so s√°nh ng√†nh theo profile; ho·∫∑c y√™u c·∫ßu g·ª£i √Ω ng√†nh ph√π h·ª£p cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.
        Y√™u c·∫ßu ki·ªÉm tra c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ ph√π h·ª£p v·ªõi t·∫ßng 1 ho·∫∑c t·∫ßng 2:
        - N·∫øu ph√π h·ª£p v·ªõi t·∫ßng 1 th√¨ tr·∫£ v·ªÅ duy nh·∫•t 1 t·ª´ "document"
        - N·∫øu ph√π h·ª£p v·ªõi t·∫ßng 2 th√¨ tr·∫£ v·ªÅ duy nh·∫•t 1 t·ª´ "recommendation"
        - N·∫øu t·∫ßng 1 ph√π h·ª£p th√¨ kh√¥ng c·∫ßn check ƒë·∫øn t·∫ßng 2
        - N·∫øu kh√¥ng ph√π h·ª£p v·ªõi t·∫ßng 1 v√† t·∫ßng 2 th√¨ tr·∫£ v·ªÅ duy nh·∫•t 1 t·ª´ "Nope"
        - Check t·∫ßng 1(document) ƒë·∫ßu ti√™n:
        - Ch·ªâ tr·∫£ v·ªÅ "document" n·∫øu N·ªòI DUNG c·ªßa document base TH·ª∞C S·ª∞ c√≥ th√¥ng tin tr·∫£ l·ªùi c√¢u h·ªèi ho·∫∑c c√¢u h·ªèi ng∆∞·ªùi d√πng ch·ªâ l√† nh·ªØng l·ªùi ch√†o.
        - Check qua t·∫ßng 2 n·∫øu:
            ‚Ä¢ ch·ªâ tr√πng t·ª´ kh√≥a nh∆∞ng kh√¥ng c√πng √Ω nghƒ©a
            ‚Ä¢ document kh√¥ng ch·ª©a d·ªØ li·ªáu c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi
            ‚Ä¢ truy v·∫•n l√† y√™u c·∫ßu t∆∞ v·∫•n c√° nh√¢n (Recommendation), kh√¥ng ph·∫£i t√¨m ki·∫øn th·ª©c
            ‚Ä¢ query chung chung nh∆∞: "t√¥i h·ª£p ng√†nh n√†o", "h√£y t∆∞ v·∫•n", "m√¥ t·∫£ v·ªÅ t√¥i", "n√™n h·ªçc g√¨"
            ‚Ä¢ context kh√¥ng cung c·∫•p th√¥ng tin tr·ª±c ti·∫øp li√™n quan
        - Check t·∫ßng 2(recommendation):
        - Ch·ªâ tr·∫£ v·ªÅ "recommendation" n·∫øu c√¢u h·ªèi ng∆∞·ªùi d√πng li√™n quan ƒë·∫øn c√°c n·ªôi dung t∆∞ v·∫•n ng√†nh h·ªçc hay t∆∞ v·∫•n cho c√° nh√¢n d·ª±a theo h·ªì s∆° c·ªßa h·ªçc sinh ho·∫∑c nh·ªØng c√¢u li√™n quan ƒë·∫øn RIASEC, h·ªçc b·∫°, GPA, s·ªü th√≠ch, nguy·ªán v·ªçng c√° nh√¢n; ho·∫∑c y√™u c·∫ßu so s√°nh ng√†nh theo profile; ho·∫∑c y√™u c·∫ßu g·ª£i √Ω ng√†nh ph√π h·ª£p
        - Ch·ªâ tr·∫£ v·ªÅ "Nope" khi c·∫£ t·∫ßng 1 v√† t·∫ßng 2 ƒë·ªÅu kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi ng∆∞·ªùi d√πng.
        
        C√¢u h·ªèi ng∆∞·ªùi d√πng: "{enriched_query}"

        N·ªôi dung Document Base (context):
        \"\"\"
        {context}
        \"\"\"

        
        """

        res = await self.llm.ainvoke(prompt)
        r = res.content.strip().lower()
        if r not in ["document", "recommendation", "nope"]:
            r = "nope"
        return r

    async def llm_suitable_for_recommedation_check(self, enriched_query: str, context: str) -> bool:
        prompt = f"""
        B·∫°n l√† h·ªá th·ªëng ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan gi·ªØa c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung t∆∞ v·∫•n ng√†nh h·ªçc hay t∆∞ v·∫•n cho c√° nh√¢n d·ª±a theo h·ªì s∆° c·ªßa h·ªçc sinh ho·∫∑c nh·ªØng c√¢u li√™n quan ƒë·∫øn RIASEC, h·ªçc b·∫°, GPA, s·ªü th√≠ch, nguy·ªán v·ªçng c√° nh√¢n; ho·∫∑c y√™u c·∫ßu so s√°nh ng√†nh theo profile; ho·∫∑c y√™u c·∫ßu g·ª£i √Ω ng√†nh ph√π h·ª£p cho chatbot RAG t∆∞ v·∫•n tuy·ªÉn sinh.

        Y√™u c·∫ßu:
        - Ch·ªâ tr·∫£ v·ªÅ "true" n·∫øu c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥.
        - Tr·∫£ v·ªÅ "false" n·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥.

        C√¢u h·ªèi ng∆∞·ªùi d√πng: "{enriched_query}"

        
        H√£y TR·∫¢ L·ªúI DUY NH·∫§T:
        - "true" ‚Üí n·∫øu c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥ 
        - "false" ‚Üí n·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn c√°c n·ªôi dung ƒë√≥
        """

        res = await self.llm.ainvoke(prompt)
        if not res.content:
            return False
        r = res.content.strip().lower()
        return ("ƒë√∫ng" in r) or ("true" in r) or (r.startswith("ƒë√∫ng")) or (r.startswith("true"))

    async def response_from_riasec_result(self, riasec_result: schemas.RiasecResultCreate):
        prompt = f"""
        B·∫°n l√† chuy√™n gia h∆∞·ªõng nghi·ªáp Holland (RIASEC).

        D∆∞·ªõi ƒë√¢y l√† ƒëi·ªÉm RIASEC c·ªßa ng∆∞·ªùi d√πng:
        - Realistic (R): {riasec_result.score_realistic}
        - Investigative (I): {riasec_result.score_investigative}
        - Artistic (A): {riasec_result.score_artistic}
        - Social (S): {riasec_result.score_social}
        - Enterprising (E): {riasec_result.score_enterprising}
        - Conventional (C): {riasec_result.score_conventional}

        Y√™u c·∫ßu:
        1. T·ª± x√°c ƒë·ªãnh m√£ RIASEC ch√≠nh c·ªßa ng∆∞·ªùi d√πng b·∫±ng c√°ch ch·ªçn 3 nh√≥m c√≥ ƒëi·ªÉm cao nh·∫•t (v√≠ d·ª•: ‚ÄúISA‚Äù, ‚ÄúREI‚Äù, ‚ÄúSEC‚Äù‚Ä¶).
        2. Gi·∫£i th√≠ch √Ω nghƒ©a m√£ RIASEC ƒë√≥ theo phong c√°ch h∆∞·ªõng nghi·ªáp.
        3. T√≥m t·∫Øt ƒë·∫∑c ƒëi·ªÉm t√≠nh c√°ch ch√≠nh (3‚Äì5 c√¢u).
        4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, s·ª≠ d·ª•ng Markdown (ti√™u ƒë·ªÅ, g·∫°ch ƒë·∫ßu d√≤ng, xu·ªëng d√≤ng r√µ r√†ng).

        Tr·∫£ v·ªÅ:
        - M·ªôt ƒëo·∫°n vƒÉn ho√†n ch·ªânh, bao g·ªìm c·∫£ m√£ RIASEC m√† b·∫°n suy lu·∫≠n.
            """

        try:
            res = await self.llm.ainvoke(prompt)
            return res.content.strip()

        except Exception as e:
            print("LLM error:", e)
            return "Xin l·ªói, h·ªá th·ªëng t·∫°m th·ªùi ch∆∞a th·ªÉ ph√¢n t√≠ch k·∫øt qu·∫£ RIASEC. B·∫°n vui l√≤ng th·ª≠ l·∫°i sau."

    async def load_session_history_to_memory(self, session_id: int, db: Session):
        memory = memory_service.get_memory(session_id)

        # L·∫•y l·ªãch s·ª≠ chat theo th·ª© t·ª± th·ªùi gian
        interactions = (
            db.query(ChatInteraction)
            .filter(ChatInteraction.session_id == session_id)
            .order_by(ChatInteraction.timestamp.asc())
            .all()
        )

        last_user_msg = None
        for inter in interactions:
            if not inter.is_from_bot:
                # user message
                last_user_msg = inter.message_text
            else:
                # bot message -> k·∫øt h·ª£p v·ªõi user message tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥)
                memory.save_context(
                    {"input": last_user_msg or ""},
                    {"output": inter.message_text}
                )
                last_user_msg = None

        # N·∫øu cu·ªëi c√πng l√† tin nh·∫Øn user ch∆∞a ƒë∆∞·ª£c ph·∫£n h·ªìi
        if last_user_msg:
            memory.save_context({"input": last_user_msg}, {"output": ""})

    def update_faq_statistics(self, db: Session, response_id: int, intent_id: int = 1):
        
        try:
            response = db.query(ChatInteraction).filter(
            ChatInteraction.interaction_id == response_id,
            ChatInteraction.is_from_bot == True
        ).first()

            if not response:
                raise ValueError("Chatbot response not found")

            faq = FaqStatistics(
                response_from_chat_id = response_id,
                intent_id = intent_id
            )
            db.add(faq)
            db.commit()

        except Exception as e:
            db.rollback()
            print(f"Error updating FaqStatistics: {e}")
            

    async def stream_response_from_context(self, query: str, context: str, session_id: int, user_id: int, intent_id: int, message: str):
        db = SessionLocal()
        
        try:
            if user_id:
                # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
                user_msg = ChatInteraction(
                    message_text=message,
                    timestamp=datetime.now(),
                    rating=None,
                    is_from_bot=False,
                    sender_id=None,
                    session_id=session_id
                )
                db.add(user_msg)
                db.flush()
            else:
                # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
                user_msg = ChatInteraction(
                    message_text=message,
                    timestamp=datetime.now(),
                    rating=None,
                    is_from_bot=False,
                    sender_id=user_id,
                    session_id=session_id
                )
                db.add(user_msg)
                db.flush()
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")
            

            prompt = f"""B·∫°n l√† m·ªôt t∆∞ v·∫•n vi√™n tuy·ªÉn sinh chuy√™n nghi·ªáp c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc FPT
            ƒê√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i tr∆∞·ªõc: 
            {chat_history}
            === TH√îNG TIN THAM KH·∫¢O ===
            {context}
            === C√ÇU H·ªéI ===
            {query}
            === H∆Ø·ªöNG D·∫™N ===
            - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
            - D·ª±a v√†o th√¥ng tin tham kh·∫£o tr√™n ƒë∆∞·ª£c cung c·∫•p
            - Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng Markdown: d√πng ti√™u ƒë·ªÅ ##, g·∫°ch ƒë·∫ßu d√≤ng -, xu·ªëng d√≤ng r√µ r√†ng.
            - H√£y t·∫°o ra c√¢u tr·∫£ l·ªùi kh√¥ng qu√° d√†i, g√≥i g·ªçn √Ω ch√≠nh, ch·ªâ khi c√¢u h·ªèi y√™u c·∫ßu "chi ti·∫øt" th√¨ m·ªõi t·∫°o c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß
            - B·∫°n l√† t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc FPT, n·∫øu th√¥ng tin c√¢u h·ªèi y√™u c√¢u t√™n 1 tr∆∞·ªùng kh√°c th√¨ h√£y n√≥i r√µ ra l√† kh√¥ng t√¨m th·∫•y th√¥ng tin
            - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ v√† g·ª£i √Ω li√™n h·ªá tr·ª±c ti·∫øp nh√¢n vi√™n t∆∞ v·∫•n
            - Kh√¥ng c·∫ßn ph·∫£i ch√†o h·ªèi m·ªói l·∫ßn tr·∫£ l·ªùi, v√†o th·∫≥ng v·∫•n ƒë·ªÅ ch√≠nh
            - N·∫øu c√¢u h·ªèi ch·ªâ l√† ch√†o h·ªèi, ho·∫∑c c√°c c√¢u x√£ giao, h√£y tr·∫£ l·ªùi b·∫±ng l·ªùi ch√†o th√¢n thi·ªán, gi·ªõi thi·ªáu v·ªÅ b·∫£n th√¢n chatbot, KH√îNG k√©o th√™m th√¥ng tin chi ti·∫øt trong context.
            - Khi c√≥ th·ªÉ, h√£y **gi·∫£i th√≠ch th√™m b·ªëi c·∫£nh ho·∫∑c g·ª£i √Ω b∆∞·ªõc ti·∫øp theo**, v√≠ d·ª•:  
                ‚ÄúB·∫°n mu·ªën m√¨nh g·ª≠i danh s√°ch ng√†nh ƒë√†o t·∫°o k√®m chuy√™n ng√†nh chi ti·∫øt kh√¥ng?‚Äù  
                ho·∫∑c  
                ‚ÄúN·∫øu b·∫°n quan t√¢m h·ªçc b·ªïng, m√¨nh c√≥ th·ªÉ n√≥i r√µ c√°c lo·∫°i h·ªçc b·ªïng hi·ªán c√≥ nh√©!‚Äù
            """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                text = chunk.content or ""
                full_response += text
                yield text
                await asyncio.sleep(0)  # Nh∆∞·ªùng event loop
            print(full_response)
            memory.save_context({"input": query}, {"output": full_response})  
            
            # === L∆∞u bot response v√†o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)
            db.flush()
            # üß© 5. Commit 1 l·∫ßn duy nh·∫•t
            db.commit()
            self.update_faq_statistics(db, bot_msg.interaction_id, intent_id = intent_id)
            print(f"üíæ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close()

    async def stream_response_from_qa(self, query: str, context: str, session_id: int = 1, user_id: int = 1, intent_id: int = 1, message: str = ""):
        db = SessionLocal()
        try:
            if user_id:
                # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
                user_msg = ChatInteraction(
                    message_text=message,
                    timestamp=datetime.now(),
                    rating=None,
                    is_from_bot=False,
                    sender_id=None,
                    session_id=session_id
                )
                db.add(user_msg)
                db.flush()
            else:
                # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
                user_msg = ChatInteraction(
                    message_text=message,
                    timestamp=datetime.now(),
                    rating=None,
                    is_from_bot=False,
                    sender_id=user_id,
                    session_id=session_id
                )
                db.add(user_msg)
                db.flush()
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")

            prompt = f"""
            B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc FPT.
            ƒê√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i tr∆∞·ªõc: 
            {chat_history}
            === C√ÇU TR·∫¢ L·ªúI CH√çNH TH·ª®C ===
            {context}

            === C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG ===
            {query}

            === H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI ===
            - H√£y ƒë·ªçc k·ªπ ph·∫ßn NG·ªÆ C·∫¢NH LI√äN QUAN, nh∆∞ng **ch·ªâ s·ª≠ d·ª•ng n√≥ n·∫øu th·∫≠t s·ª± c√≥ n·ªôi dung tr√πng kh·ªõp ho·∫∑c ph√π h·ª£p v·ªõi c√¢u h·ªèi ng∆∞·ªùi d√πng.**
            - N·∫øu ph·∫ßn C√ÇU TR·∫¢ L·ªúI CH√çNH TH·ª®C kh√¥ng li√™n quan r√µ r√†ng ƒë·∫øn c√¢u h·ªèi, **ƒë·ª´ng c·ªë tr·∫£ l·ªùi theo context** m√† h√£y n√≥i:
            ‚ÄúHi·ªán ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c cho c√¢u h·ªèi n√†y. B·∫°n c√≥ th·ªÉ n√≥i r√µ chi ti·∫øt h∆°n ƒë∆∞·ª£c kh√¥ng?‚Äù 
            - N·∫øu ph·∫ßn tr·∫£ l·ªùi ch√≠nh th·ª©c kh√¥ng ph√π h·ª£p v·ªõi c√¢u h·ªèi, h√£y n√≥i ‚ÄúHi·ªán ch∆∞a c√≥ th√¥ng tin cho c√¢u h·ªèi n√†y. Vui l√≤ng li√™n h·ªá chuy√™n vi√™n t∆∞ v·∫•n.‚Äù
            - Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng Markdown: d√πng ti√™u ƒë·ªÅ ##, g·∫°ch ƒë·∫ßu d√≤ng -, xu·ªëng d√≤ng r√µ r√†ng.
            - H√£y t·∫°o ra c√¢u tr·∫£ l·ªùi kh√¥ng qu√° d√†i, g√≥i g·ªçn √Ω ch√≠nh, ch·ªâ khi c√¢u h·ªèi y√™u c·∫ßu "chi ti·∫øt" th√¨ m·ªõi t·∫°o c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß
            - B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc FPT, nh·ªõ ki·ªÉm tra kƒ© r√µ r√†ng c√¢u h·ªèi, n·∫øu th√¥ng tin c√¢u h·ªèi y√™u c√¢u t√™n 1 tr∆∞·ªùng kh√°c th√¨ h√£y n√≥i r√µ ra l√† kh√¥ng t√¨m th·∫•y th√¥ng tin
            - N·∫øu c√¢u h·ªèi ch·ªâ l√† ch√†o h·ªèi, h·ªèi th·ªùi ti·∫øt, ho·∫∑c c√°c c√¢u x√£ giao, h√£y tr·∫£ l·ªùi b·∫±ng l·ªùi ch√†o th√¢n thi·ªán, gi·ªõi thi·ªáu v·ªÅ b·∫£n th√¢n chatbot, KH√îNG k√©o th√™m th√¥ng tin chi ti·∫øt trong context.
            - Kh√¥ng c·∫ßn ph·∫£i ch√†o h·ªèi m·ªói l·∫ßn tr·∫£ l·ªùi, v√†o th·∫≥ng v·∫•n ƒë·ªÅ ch√≠nh
            - N·∫øu c√¢u h·ªèi qu√° m∆° h·ªì, h√£y h·ªèi l·∫°i ƒë·ªÉ r√µ h∆°n v√† chi ti·∫øt h∆°n v·ªÅ c√¢u h·ªèi
            - C√≥ th·ªÉ **di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi ho·∫∑c th√¥ng tin** m·ªôt c√°ch nh·∫π nh√†ng, t·ª± nhi√™n ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ hi·ªÉu h∆°n, **nh∆∞ng tuy·ªát ƒë·ªëi kh√¥ng thay ƒë·ªïi √Ω nghƒ©a hay th√™m d·ªØ ki·ªán m·ªõi.**
            - Khi c√≥ th·ªÉ, h√£y **gi·∫£i th√≠ch th√™m b·ªëi c·∫£nh ho·∫∑c g·ª£i √Ω b∆∞·ªõc ti·∫øp theo**, v√≠ d·ª•:  
                ‚ÄúB·∫°n mu·ªën m√¨nh g·ª≠i danh s√°ch ng√†nh ƒë√†o t·∫°o k√®m chuy√™n ng√†nh chi ti·∫øt kh√¥ng?‚Äù  
                ho·∫∑c  
                ‚ÄúN·∫øu b·∫°n quan t√¢m h·ªçc b·ªïng, m√¨nh c√≥ th·ªÉ n√≥i r√µ c√°c lo·∫°i h·ªçc b·ªïng hi·ªán c√≥ nh√©!‚Äù
            """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                text = chunk.content or ""
                full_response += text
                yield text
                await asyncio.sleep(0)  # Nh∆∞·ªùng event loop

            memory.save_context({"input": query}, {"output": full_response})  
            print("Saved to memory. Current messages:", len(memory.chat_memory.messages))

            # === L∆∞u bot response v√†o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)
            db.flush()
            # üß© 5. Commit 1 l·∫ßn duy nh·∫•t
            db.commit()
            
            self.update_faq_statistics(db, bot_msg.interaction_id, intent_id = intent_id)
            print(f"üíæ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close() 
    
    async def stream_response_from_recommendation(
        self,
        user_id: int,
        session_id: int,
        query: str,
        message: str
    ):
        db = SessionLocal()
        try:
            if user_id:
                # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
                user_msg = ChatInteraction(
                    message_text=message,
                    timestamp=datetime.now(),
                    rating=None,
                    is_from_bot=False,
                    sender_id=None,
                    session_id=session_id
                )
                db.add(user_msg)
                db.flush()
            else:
                # üß© 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
                user_msg = ChatInteraction(
                    message_text=message,
                    timestamp=datetime.now(),
                    rating=None,
                    is_from_bot=False,
                    sender_id=user_id,
                    session_id=session_id
                )
                db.add(user_msg)
                db.flush()
            memory = memory_service.get_memory(session_id)
            mem_vars = memory.load_memory_variables({})
            chat_history = mem_vars.get("chat_history", "")

            user_profile = self._get_user_personality_and_academics(user_id, db)
            majors = self._get_all_majors_and_specialization_from_db(db, limit=200)

            personality = user_profile.get("personality_summary") or ""
            academic_summary = user_profile.get("academic_summary") or ""
            gpa = user_profile.get("gpa", "")

            maj_texts = []
            for m in majors:
                line = f"- [{m['major_id']}]: {m['major_name']}"
                
                if m["specializations"]:
                    for s in m["specializations"]:
                        line += f"\n    ‚Ä¢ {s['specialization_name']}"
                
                maj_texts.append(line)

            prompt = f"""
        B·∫°n l√† chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng ƒë·∫°i h·ªçc FPT. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t∆∞ v·∫•n ch·ªçn ng√†nh:
        **CH·ªà t∆∞ v·∫•n ch·ªçn ng√†nh khi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th·∫≠t s·ª± li√™n quan.**
        
        ƒê√¢y l√† ƒëo·∫°n h·ªôi tho·∫°i tr∆∞·ªõc: 
            {chat_history}
        ===========================
        ### TH√îNG TIN H·ªí S∆† NG∆Ø·ªúI D√ôNG
        Personality summary(RIASEC Result):
        {personality}

        Academic summary(h·ªçc b·∫°):
        {academic_summary}

        

        ===========================
        ### DANH S√ÅCH C√ÅC NG√ÄNH
        {chr(10).join(maj_texts)}

        ===========================
        ### C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG
        "{query}"

        ===========================
        ### H∆Ø·ªöNG D·∫™N X·ª¨ L√ù

        1. **ƒê·∫ßu ti√™n, h√£y ki·ªÉm tra xem c√¢u h·ªèi c√≥ th·∫≠t s·ª± li√™n quan ƒë·∫øn vi·ªác t∆∞ v·∫•n ch·ªçn ng√†nh hay kh√¥ng, ho·∫∑c c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn th√¥ng tin h·ªì s∆° ng∆∞·ªùi d√πng hay kh√¥ng.**
        - N·∫øu KH√îNG li√™n quan ‚Üí b·∫°n h√£y t·ª± t·∫°o c√¢u ph·∫£n h·ªìi ph√π h·ª£p v·ªõi C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG
        2. N·∫øu c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn th√¥ng tin h·ªì s∆° ng∆∞·ªùi d√πng ·ªü tr√™n bao g·ªìm RIASEC Result v√† h·ªçc b·∫° m√† h·ªì s∆° ng∆∞·ªùi d√πng tr·ªëng th√¨ h√£y y√™u c·∫ßu ng∆∞·ªùi d√πng nh·∫≠p nh·ªØng th√¥ng tin n√†y nh∆∞ RIASEC Result ho·∫∑c h·ªçc b·∫°, 1 trong 2 l√† c√≥ th·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n d·ª±a v√†o th√¥ng tin h·ªì s∆° ng∆∞·ªùi d√πng. ƒê·ªÅ xu·∫•t theo t√≠nh c√°ch c√≥ th·ªÉ d·ª±a v√†o k·∫øt qu·∫£ RIASEC Result c·ªßa TH√îNG TIN H·ªí S∆† NG∆Ø·ªúI D√ôNG
        3. Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng Markdown: d√πng ti√™u ƒë·ªÅ ##, g·∫°ch ƒë·∫ßu d√≤ng -, xu·ªëng d√≤ng r√µ r√†ng.
        4. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan th√¨ h√£y t·ª´ ch·ªëi y√™u c·∫ßu v√† ƒë·ªÅ ngh·ªã nh·∫Øn tr·ª±c ti·∫øp b√™n tuy·ªÉn sinh
        5. Kh√¥ng c·∫ßn ph·∫£i ch√†o h·ªèi m·ªói l·∫ßn tr·∫£ l·ªùi, v√†o th·∫≥ng v·∫•n ƒë·ªÅ ch√≠nh
        """
            full_response = ""
            async for chunk in self.llm.astream(prompt):
                text = chunk.content or ""
                full_response += text
                yield text
                await asyncio.sleep(0)  # Nh∆∞·ªùng event loop

            memory.save_context({"input": query}, {"output": full_response})  
            print("Saved to memory. Current messages:", len(memory.chat_memory.messages))

            # === L∆∞u bot response v√†o DB ===
            bot_msg = ChatInteraction(
                message_text=full_response,
                timestamp=datetime.now(),
                rating=None,
                is_from_bot=True,
                sender_id=None,
                session_id=session_id
            )
            db.add(bot_msg)
            db.flush()
            # üß© 5. Commit 1 l·∫ßn duy nh·∫•t
            db.commit()
            
            print(f"üíæ Saved both user+bot messages for session {session_id}")
        except SQLAlchemyError as e:
            db.rollback()
            print(f" Database error during chat transaction: {e}")
        finally:
            db.close()

    def create_training_qa(self, db: Session, intent_id: int, question: str, answer: str, created_by: int):
        qa = TrainingQuestionAnswer(
            question=question,
            answer=answer,
            intent_id=intent_id,
            created_by=created_by,
            status="draft"
        )
        db.add(qa)
        db.commit()
        db.refresh(qa)

        return qa

    def approve_training_qa(self, db: Session, qa_id: int, reviewer_id: int):
        qa = db.query(TrainingQuestionAnswer).filter_by(question_id=qa_id).first()
        if not qa:
            raise Exception("QA not found")

        if qa.status != "draft":
            raise Exception("Only draft QA can be approved")

        # embed question (answer kh√¥ng embed)
        embedding = self.embeddings.embed_query(qa.question)
        point_id = str(uuid.uuid4())

        # push to Qdrant
        self.qdrant_client.upsert(
            collection_name="training_qa",
            points=[
                PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        "question_id": qa.question_id,
                        "intent_id": qa.intent_id,
                        "question_text": qa.question,
                        "answer_text": qa.answer,
                        "type": "training_qa"
                    }
                )
            ]
        )

        # update DB
        qa.status = "approved"
        qa.approved_by = reviewer_id
        qa.approved_at = datetime.now().date()  # Convert datetime to date
        db.commit()

        return {
            "postgre_question_id": qa.question_id,
            "qdrant_question_id": point_id
        }

    def delete_training_qa(self, db: Session, qa_id: int):
        
        qa = db.query(TrainingQuestionAnswer).filter_by(question_id=qa_id).first()
        if not qa:
            raise Exception("Training QA not found")

        # X√≥a vector trong Qdrant
        self.qdrant_client.delete(
            collection_name="training_qa",
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="question_id",
                            match=models.MatchValue(value = qa_id)
                        )
                    ]
                )
            )
        )

        # X√≥a trong DB
        db.delete(qa)
        db.commit()

        return {"deleted_question_id": qa_id}

    def create_document(self, db: Session, title: str, file_path: str, intend_id: int, created_by: int):
        new_doc = KnowledgeBaseDocument(
            title=title,
            file_path=file_path,
            intend_id=intend_id,
            status="draft",
            created_by=created_by,
        )
        db.add(new_doc)
        db.commit()
        db.refresh(new_doc)

        return new_doc

    def approve_document(self, db: Session, document_id: int, reviewer_id: int, intent_id: int, metadata: dict = None):

        doc = db.query(KnowledgeBaseDocument).filter_by(document_id=document_id).first()
        if not doc:
            raise Exception("Document not found")

        if doc.status != "draft":
            raise Exception("Only draft documents can be approved")

        abs_path = os.path.abspath(doc.file_path)
        print("OPEN FILE:", abs_path)

        with open(abs_path, "rb") as f:
            file_bytes = f.read()

        # 3. Detect MIME type t·ª´ extension (DocumentProcessor c·∫ßn)
        mime_map = {
            ".pdf":  "application/pdf",
            ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            ".doc":  "application/msword",
            ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            ".xls":  "application/vnd.ms-excel",
            ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
            ".txt":  "text/plain",
        }
        ext = os.path.splitext(doc.file_path)[1].lower()
        mime_type = mime_map.get(ext, "text/plain")
        content = DocumentProcessor.extract_text(
        file_content=file_bytes,
        filename=os.path.basename(doc.file_path),
        mime_type=mime_type
        )
        # --- Split text ---
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_text(content)

        qdrant_ids = []

        # --- Save chunks to DB & Qdrant ---
        for i, chunk in enumerate(chunks):

            # # Save DocumentChunk in DB
            # db_chunk = DocumentChunk(
            #     chunk_text=chunk,
            #     document_id=document_id,
            #     created_by=reviewer_id
            # )
            # db.add(db_chunk)
            # db.flush()   # get chunk_id

            # Embed
            embedding = self.embeddings.embed_query(chunk)
            point_id = str(uuid.uuid4())

            # Push to Qdrant
            self.qdrant_client.upsert(
                collection_name="knowledge_base_documents",
                points=[
                    PointStruct(
                        id=point_id,
                        vector=embedding,
                        payload={
                            "document_id": document_id,
                            "chunk_index": i,
                            "chunk_text": chunk,
                            "intent_id": intent_id,
                            "metadata": metadata or {},
                            "type": "document"
                        }
                    )
                ]
            )

            qdrant_ids.append(point_id)

        # update document status
        doc.status = "approved"
        doc.reviewed_by = reviewer_id
        doc.reviewed_at = datetime.now().date()  # Convert datetime to date
        db.commit()

        return {
            "document_id": document_id,
            "status": doc.status
        }

    def delete_document(self, db: Session, document_id: int):
        doc = db.query(KnowledgeBaseDocument).filter_by(document_id=document_id).first()
        if not doc:
            raise Exception("Document not found")

        # X√≥a s·∫°ch vector trong Qdrant
        self.qdrant_client.delete(
            collection_name="knowledge_base_documents",
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="document_id",
                            match=models.MatchValue(value = document_id)
                        )
                    ]
                )
            )
        )

        # X√≥a chunks trong DB
        dl = db.query(DocumentChunk).filter_by(document_id=document_id)
        if dl:
            dl.delete()
        # X√≥a document trong DB
        db.delete(doc)
        db.commit()

        return {"deleted_document_id": document_id}
    



    # def add_document(self, document_id: int, content: str, intend_id: int, metadata: dict = None):
    #     text_splitter = RecursiveCharacterTextSplitter(
    #         chunk_size=1000,      # Size optimal cho Vietnamese
    #         chunk_overlap=200     # Overlap to preserve context
    #     )
    #     chunks = text_splitter.split_text(content)
        
    #     chunk_ids = []
    #     for i, chunk in enumerate(chunks):
    #         # Embed chunk
    #         embedding = self.embeddings.embed_query(chunk)
    #         point_id = str(uuid.uuid4())
            
    #         # Upsert to Qdrant
    #         self.qdrant_client.upsert(
    #             collection_name="knowledge_base_documents",
    #             points=[
    #                 PointStruct(
    #                     id=point_id,
    #                     vector=embedding,
    #                     payload={
    #                         "document_id": document_id,
    #                         "chunk_index": i,
    #                         "chunk_text": chunk,
    #                         "intend_id": intend_id,
    #                         "metadata": metadata or {},
    #                         "type": "document"
    #                     }
    #                 )
    #             ]
    #         )
    #         chunk_ids.append(point_id)
        
    #     return chunk_ids
    
    def add_training_qa(self, db: Session, intent_id: int, question_text: str, answer_text: str):
        """
        Add training Q&A pair v√†o Qdrant
        
        Ch·ªâ embed question, kh√¥ng embed answer:
        - Answer stored ·ªü DB, retrieve khi match found
        - Question d√πng ƒë·ªÉ search/match
        - Ti·∫øt ki·ªám storage, tƒÉng search speed
        
        Args:
            question_id: Primary key c·ªßa training Q&A
            intent_id: Intent n√†y thu·ªôc intent n√†o
            question_text: Question ƒë·ªÉ embed
            answer_text: Answer (l∆∞u ·ªü DB, kh√¥ng embed)
        
        Returns:
            embedding_id: Qdrant point ID
        """
        new_qa = TrainingQuestionAnswer(
            question=question_text,
            answer=answer_text,
            intent_id=1,
            created_by=1,
            status='draft'  # New Q&A starts as draft, needs review before training
        )
        db.add(new_qa)
        db.commit()
        db.refresh(new_qa)
        # Embed question text
        embedding = self.embeddings.embed_query(question_text)
        point_id = str(uuid.uuid4())
        
        # Upsert v√†o training_qa collection
        # Metadata:
        # - question_id: Link v·ªÅ DB
        # - intent_id: ƒê·ªÉ track intent stats
        # - question_text: L∆∞u original text (optional, space saving)
        # - answer_text: L∆∞u answer (retrieve khi match)
        self.qdrant_client.upsert(
            collection_name=self.training_qa_collection,
            points=[
                PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        "question_id": new_qa.question_id,
                        "intent_id": intent_id,
                        "question_text": question_text,
                        "answer_text": answer_text,
                        "type": "training_qa"
                    }
                )
            ]
        )
        
        return {
            "postgre_question_id": new_qa.question_id,
            "qdrant_question_id": point_id
        }
    
    

    def search_documents(self, query: str, top_k: int = 5):
        """
        Search documents (Fallback)
        
        Fallback path: T√¨m document chunks khi training Q&A kh√¥ng match
        - Query ‚Üí Embed ‚Üí Search documents collection
        - Return top_k chunks
        - LLM s·∫Ω synthesize answer t·ª´ chunks
        
        Args:
            query: User question
            top_k: S·ªë chunks (lower score ‚Üí fallback)
        
        Returns:
            List of document chunks
        """
        
        query_embedding = self.embeddings.embed_query(query)
        
        results = self.qdrant_client.search(
            collection_name=self.documents_collection,
            query_vector=query_embedding,
            limit=top_k
        )
        
        return results
    
    def search_training_qa(self, query: str, top_k: int = 5):
        """
        Search training Q&A (Priority 1)
        
        Fast path: T√¨m pre-approved answers
        - Query ‚Üí Embed ‚Üí Search training_qa collection
        - Return top_k matches
        - filter score > 0.8
        
        Args:
            query: User question
            top_k: S·ªë results (default 5)
        
        Returns:
            List of search results with scores
        """
        
        query_embedding = self.embeddings.embed_query(query)
        
        results = self.qdrant_client.search(
            collection_name=self.training_qa_collection,
            query_vector=query_embedding,
            limit=top_k
        )
        
        return results
    def hybrid_search(self, query: str):
        """
        Hybrid RAG Search Strategy
        
        PRIORITY SYSTEM (Cascade):
        1. TIER 1 - Training Q&A (score > 0.8)
           - Highest confidence, direct answer
           - No LLM needed, fast response
           
        2. TIER 2 - Training Q&A (0.7 < score <= 0.8)
           - Good match but not perfect
           - Use as primary answer + add document context
           
        3. TIER 3 - Document Search + LLM Generation
           - No training Q&A match
           - Search documents, LLM synthesize
           - Lower confidence, show sources
           
        4. TIER 4 - Fallback
           - Nothing found
           - Suggest live chat with officer
        
        Returns:
            {
                "response": str,
                "response_source": "training_qa" | "document" | "fallback",
                "confidence": float,
                "top_match": obj,
                "intent_id": int,
                "sources": list
            }
        """
        
        # STEP 1: Search training Q&A
        qa_results = self.search_training_qa(query, top_k=3)
        
        # TIER 1: Perfect match (score > 0.7)
        if qa_results and qa_results[0].score > 0.7:
            top_match = qa_results[0]
            return {
                "response_official_answer": top_match.payload.get("answer_text"),
                "response_source": "training_qa",
                "confidence": top_match.score,
                "top_match": top_match,
                "intent_id": top_match.payload.get("intent_id"),
                "question_id": top_match.payload.get("question_id"),
                "sources": []
            }
        
        
        # TIER 2: No training Q&A match, try documents
        doc_results = self.search_documents(query, top_k=5)
        if doc_results and len(doc_results) > 0: 
            return {
                    "response": doc_results,
                    "response_source": "document",
                    "confidence": doc_results[0].score,
                    "top_match": doc_results[0],
                    "intent_id": doc_results[0].payload.get("intent_id"),
                    "sources": [r.payload.get("document_id") for r in doc_results]
                }
        else:
            return {
                "response": doc_results,
                "response_source": "document",
                "confidence": 0.0,
                "top_match": None,
                "intent_id": 0,
                "sources": []
            }
        
    def _get_user_personality_and_academics(self, user_id: int, db: Session) -> Dict[str, Any]:
        out = {
            "personality_summary": None,
            "riasec": None,
            "academic_summary": None,
            "gpa": None,
            "subjects": {}
        }

        # --- RIASEC result ---
        ri = (
            db.query(RiasecResult)
            .filter(RiasecResult.customer_id == user_id)
            .order_by(RiasecResult.result_id.desc())
            .first()
        )

        if ri:
            out["riasec"] = {
                "R": ri.score_realistic,
                "I": ri.score_investigative,
                "A": ri.score_artistic,
                "S": ri.score_social,
                "E": ri.score_enterprising,
                "C": ri.score_conventional,
            }
            # `result` field = summary c·ªßa b·∫°n
            out["personality_summary"] = ri.result or self._riasec_to_summary(out["riasec"])

        # --- Academic scores ---
        score = (
            db.query(AcademicScore)
            .filter(AcademicScore.customer_id == user_id)
            .first()
        )

        if score:
            subj_map = {
            "math": score.math,
            "literature": score.literature,
            "english": score.english,
            "physics": score.physics,
            "chemistry": score.chemistry,
            "biology": score.biology,
            "history": score.history,
            "geography": score.geography,
        }

            # simple GPA = average score
            valid_scores = [v for v in subj_map.values() if v is not None]
            gpa = round(sum(valid_scores) / len(valid_scores), 2)

            out["subjects"] = subj_map
            out["gpa"] = gpa
            out["academic_summary"] = (
                f"GPA x·∫•p x·ªâ {gpa}. C√°c m√¥n: " +
                ", ".join([f"{k}: {v}" for k, v in subj_map.items()])
            )
            print(out["academic_summary"])
        return out

    def _riasec_to_summary(self, ri_map: Dict[str,int]) -> str:
        # very small helper - b·∫°n c√≥ th·ªÉ m·ªü r·ªông
        order = sorted(ri_map.items(), key=lambda x: -x[1])
        top = order[0][0] if order else None
        return f"∆Øu th·∫ø RIASEC: {', '.join([f'{k}={v}' for k,v in ri_map.items()])}. Ch√≠nh: {top}."

    def _get_all_majors_from_db(self, db: Session, limit: int = 200) -> List[Dict[str,Any]]:
        """
        L·∫•y danh s√°ch majors
        """
        rows = db.query(Major).order_by(Major.major_name).limit(limit).all()
        majors = []
        for r in rows:
            majors.append({
                "major_id": r.major_id,
                "major_name": r.major_name,
            })
        return majors

    def _get_all_majors_and_specialization_from_db(self, db: Session, limit: int = 200) -> List[Dict[str, Any]]:
        """
        L·∫•y danh s√°ch majors k√®m theo danh s√°ch specializations
        """
        rows = (
            db.query(Major)
            .order_by(Major.major_name)
            .limit(limit)
            .all()
        )

        majors = []
        for r in rows:
            majors.append({
                "major_id": r.major_id,
                "major_name": r.major_name,
                "specializations": [
                    {
                        "specialization_id": s.specialization_id,
                        "specialization_name": s.specialization_name
                    }
                    for s in r.specializations
                ]
            })

        return majors
    
    

    

langchain_service = TrainingService()
