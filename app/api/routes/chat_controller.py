from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from fastapi.security import OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from typing import List, Optional
import uuid
import asyncio
import json
from app.services.training_service import TrainingService
from pathlib import Path

router = APIRouter()
# @router.post("/chat")
# async def chat(
#         message: str
# ):
#     doc_results = langchain_service.search_documents(message, top_k=5)
#             # Build context t·ª´ documents
#     context = "\n\n".join([r.payload.get("chunk_text", "") for r in doc_results])
            
#             # Generate response using LLM
#     generated_response = langchain_service.generate_response_from_context(message, context)
            
#     return {
#             "response": generated_response,
#             "response_source": "document",
#             "confidence": doc_results[0].score,
#             "top_match": None,
#             "intent_id": None,
#             "sources": [r.payload.get("document_id") for r in doc_results]
#             }
#th√™m 2 t·∫ßng check chat
@router.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    await websocket.accept()
    greeting_chunks = ["Ch√†o b·∫°n! üëã M√¨nh l√† Chatbot t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa tr∆∞·ªùng XYZ.", "R·∫•t vui ƒë∆∞·ª£c ƒë·ªìng h√†nh c√πng b·∫°n!\nM√¨nh c√≥ th·ªÉ gi√∫p b·∫°n:","\n\n 1Ô∏è‚É£ Gi·ªõi thi·ªáu c√°c ng√†nh h·ªçc, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o v√† ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t c·ªßa t·ª´ng ng√†nh.\n\n", "2Ô∏è‚É£ T∆∞ v·∫•n l·ªô tr√¨nh h·ªçc t·∫≠p, c∆° h·ªôi ngh·ªÅ nghi·ªáp v√† k·ªπ nƒÉng c·∫ßn c√≥ cho t·ª´ng ng√†nh.\n\n", "3Ô∏è‚É£ Cung c·∫•p th√¥ng tin tuy·ªÉn sinh: ƒëi·ªÅu ki·ªán, h·ªì s∆°, m·ªëc th·ªùi gian quan tr·ªçng.\n\n", "4Ô∏è‚É£ H∆∞·ªõng d·∫´n tham gia c√°c ho·∫°t ƒë·ªông tr·∫£i nghi·ªám, c√¢u l·∫°c b·ªô, th·ª±c t·∫≠p v√† h·ªçc b·ªïng.\n\n", "5Ô∏è‚É£ Gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ c∆° s·ªü v·∫≠t ch·∫•t, k√Ω t√∫c x√°, v√† c√°c d·ªãch v·ª• h·ªó tr·ª£ sinh vi√™n.\n\nB·∫°n mu·ªën b·∫Øt ƒë·∫ßu t√¨m hi·ªÉu v·ªÅ lƒ©nh v·ª±c hay ng√†nh h·ªçc n√†o tr∆∞·ªõc? üòÑ"]
    for chunk in greeting_chunks:
        await websocket.send_text(json.dumps({"event": "chunk", "content": chunk}))
        await asyncio.sleep(0.01)  # delay ng·∫Øn ƒë·ªÉ client hi·ªÉn th·ªã m∆∞·ª£t
    
    await websocket.send_text(json.dumps({"event": "done", "sources": [], "confidence": 1.0}))
    try:
        while True:
            # Nh·∫≠n tin nh·∫Øn t·ª´ client
            data = await websocket.receive_text()
            message = json.loads(data).get("message", "")
        
            
            # T√¨m context li√™n quan
            # doc_results = TrainingService.search_documents(message, top_k=5)
            service = TrainingService()
            result  = service.hybrid_search(message)
            
            tier_source = result.get("response_source")
            confidence = result.get("confidence", 0.0)

            # === TIER 1: training_qa - score > 0.8 ===
            if tier_source == "training_qa" and confidence > 0.8:
                print("floor 1")
                response_text = result["response_official_answer"]

                await websocket.send_text(json.dumps({
                    "event": "chunk",
                    "content": response_text
                }))
                await websocket.send_text(json.dumps({
                    "event": "done",
                    "sources": result.get("sources", []),
                    "confidence": confidence
                }))
                continue
            # === TIER 2: hybrid (0.7 < score <= 0.8) ===
            # elif result["response_source"] == "training_qa" and result["confidence"] > 0.7:
            #     print("floor 2")
            #     async for chunk in service.stream_response_from_hybrid(
            #         query=message,
            #         official_answer=result["response_official_answer"],
            #         additional_context=result.get("additional_context", "")
            #     ):
            #         await websocket.send_text(json.dumps({
            #             "event": "chunk",
            #             "content": getattr(chunk, "content", str(chunk))
            #         }))
            #     try:
            #         await websocket.send_text(json.dumps({
            #             "event": "done",
            #             "confidence": confidence
            #         }))
            #     except Exception:
            #         print("Kh√¥ng th·ªÉ g·ª≠i event done v√¨ client ƒë√£ ng·∫Øt.")
            #         break
            # === TIER 3: document-only (no QA match) ===
            else:
                print("floor 3")
                context = "\n\n".join([r.payload.get("chunk_text", "") for r in result["response"]])
                service = TrainingService()
                # Stream ph·∫£n h·ªìi t·ª´ng ph·∫ßn
                async for chunk in service.stream_response_from_context(message, context):
                    # chunk c√≥ th·ªÉ l√† str ho·∫∑c object tu·ª≥ model ‚Üí √©p v·ªÅ text
                    content = getattr(chunk, "content", None) or str(chunk)
                    
                    # G·ª≠i JSON ƒë·ªÉ client d·ªÖ parse
                    await websocket.send_text(json.dumps({
                        "event": "chunk",
                        "content": content
                    }))

            # G·ª≠i t√≠n hi·ªáu k·∫øt th√∫c khi ho√†n t·∫•t
                try:
                    await websocket.send_text(json.dumps({
                        "event": "done",
                        "sources": result.get("sources", []),
                        "confidence": confidence
                    }))
                except Exception:
                    print("Kh√¥ng th·ªÉ g·ª≠i event done v√¨ client ƒë√£ ng·∫Øt.")
                    break
    except WebSocketDisconnect:
        print("Client disconnected")


            











# router.post("/chat")
# async def chat(
#     message: str,
#     # current_user: Users = Depends(get_current_user),
#     # db: Session = Depends(get_db)
# ):
#     """
#     Main Chat Endpoint - Hybrid RAG Pipeline
    
#     FLOW:
#     1. Session Management: Get/create session
#     2. Hybrid Search: Training Q&A > Documents > Fallback
#     3. Intent Detection: Track conversation topics
#     4. Database Logging: Save interaction + stats
#     5. Response Return: With confidence & source info
    
#     Response Source Types:
#     - "training_qa": Direct answer t·ª´ consultant (confidence >= 0.7)
#     - "document": LLM-generated t·ª´ documents (confidence >= 0.6)
#     - "fallback": Generic answer (no match found)
    
#     Confidence Interpretation:
#     - >= 0.8: R·∫•t t·ª± tin, c√≥ th·ªÉ show tr·ª±c ti·∫øp
#     - 0.7-0.8: T·ª± tin, nh∆∞ng n√™n offer live chat option
#     - 0.6-0.7: B√¨nh th∆∞·ªùng, definitely show sources
#     - < 0.6: Kh√¥ng t·ª± tin, suggest live chat
#     """
    
#     # STEP 1: SESSION MANAGEMENT
#     session_id = message.session_id or str(uuid.uuid4())
    
#     # Get or create session
#     # session_type = "chatbot" (not live chat)
#     chat_session = db.query(AdmissionOfficialChatSession).filter(
#         AdmissionOfficialChatSession.session_id == session_id
#     ).first()
    
#     if not chat_session:
#         chat_session = AdmissionOfficialChatSession(
#             session_id=session_id,
#             user_id=current_user.id,
#             session_type="chatbot"  # Start as chatbot, can convert to live
#         )
#         db.add(chat_session)
#         db.commit()
    
#     # STEP 2: HYBRID RAG SEARCH
#     # Orchestrator function g·ªçi:
#     # - search_training_qa() ‚Üí Tier 1
#     # - search_documents() ‚Üí Tier 2-3
#     # - generate_response_from_context() ‚Üí Tier 2-3
#     # T·ª± ƒë·ªông fallback n·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c
#     # search_result = rag_service.hybrid_search(message.message)
#     search_result = langchain_service.hybrid_search(message.message)

#     response_text = search_result.get("response")
#     response_source = search_result.get("response_source")  # training_qa/document/fallback
#     confidence = search_result.get("confidence", 0.0)
#     intent_id = search_result.get("intent_id")
#     question_id = search_result.get("question_id")
#     sources = search_result.get("sources", [])
    
#     # STEP 3: SAVE INTERACTION
#     # Log m·ªói interaction ƒë·ªÉ:
#     # - Build conversation history
#     # - Analytics/improvement
#     # - User feedback tracking
#     interaction = ChatInteraction(
#         session_id=session_id,
#         user_id=current_user.id,
#         user_message=message.message,
#         bot_response=response_text,
#         intent_detected=None,  # Will fill from intent table if found
#         confidence_score=confidence,
#         response_source=response_source,
#         sources_used=json.dumps(sources),
#         training_qa_id=question_id
#     )
#     db.add(interaction)
    
#     # STEP 4: UPDATE FAQ STATISTICS
#     # Track: C√¢u h·ªèi n√†o ƒë∆∞·ª£c h·ªèi nhi·ªÅu nh·∫•t?
#     # D√πng ƒë·ªÉ identify missing Q&A, improve knowledge base
#     if intent_id:
#         # Get intent name t·ª´ database
#         intent_obj = db.query(Intent).filter(Intent.intent_id == intent_id).first()
#         if intent_obj:
#             interaction.intent_detected = intent_obj.name
            
#             # Update FAQ stats
#             faq_stat = db.query(FaqStatistics).filter(
#                 FaqStatistics.intent_id == intent_id
#             ).first()
            
#             if faq_stat:
#                 faq_stat.count += 1
#                 faq_stat.last_asked = datetime.utcnow()
#             else:
#                 faq_stat = FaqStatistics(
#                     intent_id=intent_id,
#                     question_text=message.message,
#                     count=1
#                 )
#                 db.add(faq_stat)
    
#     db.commit()
    
#     # STEP 5: RETURN RESPONSE
#     return ChatResponse(
#         response=response_text,
#         intent=interaction.intent_detected,
#         confidence=confidence,
#         response_source=response_source,
#         sources=[{"id": s, "type": "document"} for s in sources],
#         session_id=session_id
#     )